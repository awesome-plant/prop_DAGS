{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNU https://spys.one/en/https-ssl-proxy/\n",
    "# DNU http://free-proxy.cz/en/proxylist/country/all/http/ping/level2/2\n",
    "# DNU http://www.freeproxylists.net/?c=&pt=&pr=&a%5B%5D=1&a%5B%5D=2&u=50\n",
    "# -DONE https://openproxy.space/list\n",
    "# -DONE https://proxyscrape.com/free-proxy-list\n",
    "# -DONE https://www.proxy-list.download/\n",
    "# DONE https://www.proxynova.com/proxy-server-list/anonymous-proxies/\n",
    "# https://www.proxyscan.io/\n",
    "# https://free-proxy-list.net/anonymous-proxy.html\n",
    "# https://www.hidemyass-freeproxy.com/ ? \n",
    "# https://nl.hideproxy.me/ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "from psycopg2 import Error\n",
    "import csv\n",
    "from io import StringIO\n",
    "def getProxies(ps_user, ps_pass, ps_host, ps_port, ps_db, sql_start, sql_size):\n",
    "    import pandas as pd\n",
    "    #queries db and returns list of all proxies within paramaters \n",
    "    with psycopg2.connect(user=ps_user,password=ps_pass,host=ps_host,port=ps_port,database=ps_db) as conn:\n",
    "        check_proxy_list=pd.read_sql_query(\"SELECT proxy FROM sc_land.sc_proxy_raw order by table_id limit \" + str(sql_size) + \" offset \" + str(sql_start), conn)\n",
    "        print(\"got row list, start:\", str(sql_start), 'length:', str(sql_size))\n",
    "    return check_proxy_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "got row list, start: 0 length: 2\n"
     ]
    }
   ],
   "source": [
    "check_proxy_list = getProxies(\n",
    "        ps_user=\"postgres\"\n",
    "        , ps_pass=\"root\"\n",
    "        , ps_host=\"172.22.114.65\"\n",
    "        , ps_port=\"5432\"\n",
    "        , ps_db=\"scrape_db\"\n",
    "        , sql_start=0\n",
    "        , sql_size=2\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_proxy_list['status', 'error'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testProxy(proxy, timeout, my_ip, **kwargs):\n",
    "    # def here returns proxy, confirmed with different whatismyip return \n",
    "    #return true when dif\n",
    "    result=False\n",
    "    error=None\n",
    "    headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "    proxies= { 'http': 'http://' + proxy, 'https': 'https://' + proxy } \n",
    "    url='https://ident.me/'\n",
    "    try:\n",
    "        r = requests.get(url, headers=headers, proxies=proxies, timeout=timeout)\n",
    "        _newIP = r.text\n",
    "        if my_ip !=_newIP: #IP masked\n",
    "            try: \n",
    "                site_url='https://www.realestate.com.au/'\n",
    "                r = requests.get(site_url, headers=headers, proxies=proxies, timeout=timeout)\n",
    "                result=True\n",
    "            except Exception as e:\n",
    "                error = url + '-' + str(e) \n",
    "        else: error = url + '-no IP mask'\n",
    "    except Exception as e: \n",
    "        error = url + '-' + str(e) \n",
    "    print(\"proxy:\", proxy, '-result:', result, '-error:', error)\n",
    "    return result, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "myIP is: 220.253.123.147\n",
      "proxy: 14.97.2.107:80 -result: False -error: https://ident.me/-HTTPSConnectionPool(host='ident.me', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x000002056E2D7EC8>, 'Connection to 14.97.2.107 timed out. (connect timeout=3)'))\n",
      "proxy: 51.158.119.88:8811 -result: False -error: https://ident.me/-HTTPSConnectionPool(host='ident.me', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x000002056E2FF208>, 'Connection to 51.158.119.88 timed out. (connect timeout=3)'))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "myIP='220.253.123.147'\n",
    "print(\"myIP is:\", myIP)\n",
    "\n",
    "#now we check they work\n",
    "check_proxy_list['status'],check_proxy_list['error'] = check_proxy_list['proxy'].apply(lambda x: testProxy(proxy=x,timeout=3, my_ip=myIP) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "proxy: 14.97.2.107:80 -result: False -error: https://ident.me/-HTTPSConnectionPool(host='ident.me', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x000002056E309508>, 'Connection to 14.97.2.107 timed out. (connect timeout=3)'))\nFalse https://ident.me/-HTTPSConnectionPool(host='ident.me', port=443): Max retries exceeded with url: / (Caused by ConnectTimeoutError(<urllib3.connection.VerifiedHTTPSConnection object at 0x000002056E309508>, 'Connection to 14.97.2.107 timed out. (connect timeout=3)'))\n"
     ]
    }
   ],
   "source": [
    "status, error = testProxy(proxy='14.97.2.107:80',timeout=3, my_ip=myIP)\n",
    "print(status, error )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "proxy list cols: proxy    object\n          ...  \nerror    object\nLength: 3, dtype: object\n                proxy                                             status  \\\n0      14.97.2.107:80                                              False   \n1  51.158.119.88:8811  https://ident.me/-HTTPSConnectionPool(host='id...   \n\n                                               error  \n0                                              False  \n1  https://ident.me/-HTTPSConnectionPool(host='id...  \n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_rows', len(check_proxy_list), 'display.max_columns', None):  # more options can be specified also\n",
    "    print(\"proxy list cols:\", check_proxy_list.dtypes)\n",
    "    print(check_proxy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "220.253.123.147\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time \n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "url='https://ident.me/'\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")\n",
    "browser = webdriver.Chrome(executable_path=r\"C:\\Users\\chapo\\Downloads\\chromedriver.exe\", chrome_options=chrome_options)\n",
    "browser.get(url)\n",
    "\n",
    "IP = re.sub('<[^<]+?>', '', browser.page_source)\n",
    "print(IP)\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<html><head></head><body><pre style=\"word-wrap: break-word; white-space: pre-wrap;\">220.253.123.147</pre></body></html>\n220.253.123.147\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "IP = re.sub('<[^<]+?>', '', browser.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "https://ident.me/ HTTPSConnectionPool(host='ident.me', port=443): Max retries exceeded with url: / (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response')))\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "timeout =3\n",
    "proxy = '58.147.170.132:10801'\n",
    "result=''\n",
    "\n",
    "\n",
    "_actualIP=q.text\n",
    "_newIP=_actualIP\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "proxies= { 'http': 'http://' + proxy, 'https': 'https://' + proxy } \n",
    "url='https://ident.me/'\n",
    "q=requests.get(url, headers=headers)\n",
    "\n",
    "try:\n",
    "    r = requests.get(url, headers=headers, proxies=proxies, timeout=timeout)\n",
    "    _newIP = r.text\n",
    "    if _actualIP !=_newIP: #IP masked\n",
    "        print(\"worked\")\n",
    "        try: \n",
    "            \n",
    "            site_url='https://www.realestate.com.au/'\n",
    "            r = requests.get(site_url, headers=headers, proxies=proxies, timeout=timeout)\n",
    "            # print(\"IP:\", proxy, \"-capable of scraping:\",site_url)\n",
    "            result=True\n",
    "        except Exception as e:\n",
    "            result='error: ' + str(e)\n",
    "    else: print(\"wrong IP:\", r.text)\n",
    "except Exception as e:\n",
    "    print(url, e) \n",
    "    # pass\n",
    "# if result==True: print(\"IP:\", proxy, \"-capable of scraping:\",site_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime \n",
    "import time \n",
    "import random\n",
    "baseurl='https://www.realestate.com.au/xml-sitemap/'\n",
    "        # , 'RootDir': '/opt/airflow/logs/XML_save_folder' \n",
    "PageSaveFolder=r'D:\\shared_folder\\airflow_logs\\XML_save_folder\\gz_files'#'/opt/airflow/logs/XML_save_folder/raw_sitemap/'\n",
    "Scrapewait=5\n",
    "# def ScrapeURL(baseurl, PageSaveFolder, Scrapewait, **kwargs):  \n",
    "XMLsaveFile=\"XML_scrape_\" + (datetime.datetime.now()).strftime('%Y-%m-%d')\n",
    "time.sleep(random.randint(1,10))\n",
    "# ua = UserAgent()\n",
    "#headers = {'User-Agent':str(ua.random)}\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "url='https://free-proxy-list.net/anonymous-proxy.html'\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")\n",
    "browser = webdriver.Chrome(executable_path=r\"C:\\Users\\chapo\\Downloads\\chromedriver.exe\", chrome_options=chrome_options)\n",
    "browser.get(url)\n",
    "\n",
    "browser.find_element_by_xpath(\"//select[@name='proxylisttable_length']/option[text()='80']\").click()\n",
    "\n",
    "table_MN = pd.read_html(browser.page_source)\n",
    "browser.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         IP Address     Port Code         Country    Anonymity Google Https  \\\n",
       "0    185.198.189.21   8080.0   GB  United Kingdom    anonymous     no   yes   \n",
       "1    150.129.148.99  35101.0   IN           India  elite proxy     no   yes   \n",
       "2   185.236.203.208   3128.0   DK         Denmark  elite proxy     no   yes   \n",
       "3      41.65.146.38   8080.0   EG           Egypt    anonymous     no   yes   \n",
       "4    181.129.183.19  53281.0   CO        Colombia  elite proxy     no   yes   \n",
       "..              ...      ...  ...             ...          ...    ...   ...   \n",
       "75  144.217.101.245   3129.0   CA          Canada  elite proxy     no   yes   \n",
       "76    51.158.119.88   8811.0   FR          France    anonymous     no   yes   \n",
       "77   189.146.126.85     80.0   MX          Mexico  elite proxy     no    no   \n",
       "78     51.75.147.44   3128.0   FR          France    anonymous     no    no   \n",
       "79  202.141.233.166  48995.0   PK        Pakistan  elite proxy     no   yes   \n",
       "\n",
       "      Last Checked  \n",
       "0   23 seconds ago  \n",
       "1   23 seconds ago  \n",
       "2   23 seconds ago  \n",
       "3   23 seconds ago  \n",
       "4   23 seconds ago  \n",
       "..             ...  \n",
       "75    1 minute ago  \n",
       "76    1 minute ago  \n",
       "77    1 minute ago  \n",
       "78    1 minute ago  \n",
       "79    1 minute ago  \n",
       "\n",
       "[80 rows x 8 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>IP Address</th>\n      <th>Port</th>\n      <th>Code</th>\n      <th>Country</th>\n      <th>Anonymity</th>\n      <th>Google</th>\n      <th>Https</th>\n      <th>Last Checked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>185.198.189.21</td>\n      <td>8080.0</td>\n      <td>GB</td>\n      <td>United Kingdom</td>\n      <td>anonymous</td>\n      <td>no</td>\n      <td>yes</td>\n      <td>23 seconds ago</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>150.129.148.99</td>\n      <td>35101.0</td>\n      <td>IN</td>\n      <td>India</td>\n      <td>elite proxy</td>\n      <td>no</td>\n      <td>yes</td>\n      <td>23 seconds ago</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>185.236.203.208</td>\n      <td>3128.0</td>\n      <td>DK</td>\n      <td>Denmark</td>\n      <td>elite proxy</td>\n      <td>no</td>\n      <td>yes</td>\n      <td>23 seconds ago</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>41.65.146.38</td>\n      <td>8080.0</td>\n      <td>EG</td>\n      <td>Egypt</td>\n      <td>anonymous</td>\n      <td>no</td>\n      <td>yes</td>\n      <td>23 seconds ago</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>181.129.183.19</td>\n      <td>53281.0</td>\n      <td>CO</td>\n      <td>Colombia</td>\n      <td>elite proxy</td>\n      <td>no</td>\n      <td>yes</td>\n      <td>23 seconds ago</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>144.217.101.245</td>\n      <td>3129.0</td>\n      <td>CA</td>\n      <td>Canada</td>\n      <td>elite proxy</td>\n      <td>no</td>\n      <td>yes</td>\n      <td>1 minute ago</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>51.158.119.88</td>\n      <td>8811.0</td>\n      <td>FR</td>\n      <td>France</td>\n      <td>anonymous</td>\n      <td>no</td>\n      <td>yes</td>\n      <td>1 minute ago</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>189.146.126.85</td>\n      <td>80.0</td>\n      <td>MX</td>\n      <td>Mexico</td>\n      <td>elite proxy</td>\n      <td>no</td>\n      <td>no</td>\n      <td>1 minute ago</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>51.75.147.44</td>\n      <td>3128.0</td>\n      <td>FR</td>\n      <td>France</td>\n      <td>anonymous</td>\n      <td>no</td>\n      <td>no</td>\n      <td>1 minute ago</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>202.141.233.166</td>\n      <td>48995.0</td>\n      <td>PK</td>\n      <td>Pakistan</td>\n      <td>elite proxy</td>\n      <td>no</td>\n      <td>yes</td>\n      <td>1 minute ago</td>\n    </tr>\n  </tbody>\n</table>\n<p>80 rows Ã— 8 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "table_MN[0][table_MN[0]['IP Address'].notnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docker script\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver import ActionChains\n",
    "import time \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_prefs = {}\n",
    "chrome_options.experimental_options[\"prefs\"] = chrome_prefs\n",
    "chrome_prefs[\"profile.default_content_settings\"] = {\"images\": 2}\n",
    "\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "url='https://openproxy.space/list'\n",
    "browser = webdriver.Chrome(options=chrome_options)\n",
    "browser.get(url)\n",
    "browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "#get subpage urls \n",
    "browser.implicitly_wait(10)\n",
    "time.sleep(5)\n",
    "ListlinkerHref = browser.find_elements_by_xpath(\"//*[@href]\")\n",
    "proxylist=[] #stores IPs \n",
    "webpage=[] #\n",
    "proxy_list=[] #stores IP pages \n",
    "count=0\n",
    "#get proxy pages\n",
    "for proxy_page in ListlinkerHref:\n",
    "    if \"FRESH\" in proxy_page.text :\n",
    "        print(count, proxy_page.get_attribute(\"href\"))\n",
    "        proxy_list.append(proxy_page.get_attribute(\"href\"))\n",
    "    count+=1\n",
    "#now we get the proxies themselves\n",
    "for proxy_page in proxy_list:\n",
    "    print(\"scraping page:\", proxy_page)\n",
    "    browser.get(proxy_page)\n",
    "    browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "    #get subpage urls \n",
    "    browser.implicitly_wait(10)\n",
    "    time.sleep(5)\n",
    "    s_scrape = browser.find_element_by_css_selector(\"textarea\") \n",
    "    for IP in (s_scrape.text).splitlines(): #add to list \n",
    "        proxylist.append(IP)\n",
    "        webpage.append(proxy_page)\n",
    "\n",
    "print(\"done scraping\")\n",
    "#now write to df \n",
    "df_proxy_list = pd.DataFrame(\n",
    "    np.column_stack([proxylist, webpage]), \n",
    "    columns=['proxy','webpage'])\n",
    "print(df_proxy_list.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "import requests \n",
    "def getProxy(ps_user, ps_pass, ps_host, ps_port, ps_db, update, **kwargs): \n",
    "    status=False\n",
    "    proxy=''\n",
    "    try: \n",
    "        with psycopg2.connect(user=ps_user,password=ps_pass,host=ps_host,port=ps_port,database=ps_db) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(\"select proxy from sc_land.sc_proxy_raw where status ='ready' order by table_id limit 1\")\n",
    "                result = cur.fetchone()\n",
    "                if update==True:\n",
    "                    cur.execute(\"update sc_land.sc_proxy_raw set status = 'used' where proxy = %(proxy)s\",\n",
    "                        {\n",
    "                            'proxy': result[0]\n",
    "                        }\n",
    "                    )\n",
    "                    conn.commit()\n",
    "        print(\"proxy used is:\", result[0])\n",
    "        proxy=result[0]\n",
    "        status=True\n",
    "    except Exception as e: \n",
    "        print(\"error on get next proxy:\", e)\n",
    "    return proxy, status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "proxy used is: 188.241.45.61:4145\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ProxyError",
     "evalue": "HTTPSConnectionPool(host='www.realestate.com.au', port=443): Max retries exceeded with url: /xml-sitemap/ (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response')))",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteDisconnected\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    661\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_new_proxy_conn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 662\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prepare_proxy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    663\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_prepare_proxy\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m    947\u001b[0m         \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_tunnel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_proxy_host\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproxy_headers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 948\u001b[1;33m         \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    307\u001b[0m             \u001b[1;31m# self._tunnel_host below.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tunnel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    309\u001b[0m             \u001b[1;31m# Mark this connection as not reusable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_tunnel\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    915\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresponse_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 916\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    917\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    274\u001b[0m             \u001b[1;31m# sending a valid response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m             raise RemoteDisconnected(\"Remote end closed connection without\"\n\u001b[0m\u001b[0;32m    276\u001b[0m                                      \" response\")\n",
      "\u001b[1;31mRemoteDisconnected\u001b[0m: Remote end closed connection without response",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m                 )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    719\u001b[0m             retries = retries.increment(\n\u001b[1;32m--> 720\u001b[1;33m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    721\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py\u001b[0m in \u001b[0;36mincrement\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    435\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 436\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='www.realestate.com.au', port=443): Max retries exceeded with url: /xml-sitemap/ (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response')))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProxyError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-d24983dc6812>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m \u001b[1;34m'http'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'http://'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mproxy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'https'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'https://'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mproxy\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbaseurl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    528\u001b[0m         }\n\u001b[0;32m    529\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 643\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ProxyError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mProxyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_SSLError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mProxyError\u001b[0m: HTTPSConnectionPool(host='www.realestate.com.au', port=443): Max retries exceeded with url: /xml-sitemap/ (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response')))"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "proxy, status=getProxy(\"postgres\", \"root\", \"172.22.114.65\", \"5432\", \"scrape_db\", True)\n",
    "proxies= { 'http': 'http://' + proxy, 'https': 'https://' + proxy } \n",
    "timeout=3\n",
    "requests.get(baseurl, headers=headers, proxies=proxies, timeout=timeout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "proxy used is: 204.101.61.82:4145\n2021-02-07 22:01:46\ntrying to scrape with:  204.101.61.82:4145\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "session = requests.Session()\n",
    "retry = Retry(connect=3, backoff_factor=0.5)\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount('http://', adapter)\n",
    "session.mount('https://', adapter)\n",
    "\n",
    "# response = session.get(baseurl,headers=headers,proxies=proxies, timeout=Scrapewait)\n",
    "\n",
    "scrape_stat=False\n",
    "lc=0\n",
    "\n",
    "while scrape_stat==False:\n",
    "    lc+=1\n",
    "    proxy, status=getProxy(\"postgres\", \"root\", \"172.22.114.65\", \"5432\", \"scrape_db\", True)\n",
    "    proxies= { 'http': 'http://' + proxy, 'https': 'https://' + proxy } \n",
    "    H_ScrapeDT=(datetime.datetime.now())\n",
    "\n",
    "    print( (datetime.datetime.now()).strftime('%Y-%m-%d %H:%M:%S') )\n",
    "    try:\n",
    "        print(\"trying to scrape with: \", proxy)\n",
    "        response = session.get(baseurl,headers=headers,proxies=proxies, timeout=Scrapewait)\n",
    "        xmlFile=PageSaveFolder + XMLsaveFile \n",
    "        saveXML=open(xmlFile +'.xml', \"w\")\n",
    "        saveXML.close()\n",
    "        tree = etree.fromstring(response.content) \n",
    "        scrape_stat=True\n",
    "    except Exception as e: \n",
    "        print(\"try:\", str(lc),\"- error:\", str(e))\n",
    "        \n",
    "print(\"fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(xmlFile +'.xml', \"wb\") as saveXML:\n",
    "    saveXML.write(etree.tostring(tree,pretty_print=True))\n",
    "print(\"temp file saved to: \" + xmlFile +'.xml')\n",
    "H_FileSize=round(os.path.getsize(xmlFile +'.xml') / 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import pandas as pd\n",
    "import time \n",
    "import lxml \n",
    "import requests\n",
    "import base64\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from tkinter import Tk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "session = requests.Session()\n",
    "retry = Retry(connect=5, backoff_factor=0.5)\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount('http://', adapter)\n",
    "session.mount('https://', adapter)\n",
    "url='https://www.proxynova.com/proxy-server-list/anonymous-proxies/'\n",
    "response = session.get(url, headers=headers)#proxies={'http:' : 'http://139.224.47.77:8080', 'https:' : 'http://139.224.47.77:8080'})\n",
    "#proxy nova\n",
    "df_pn = pd.read_html(response.text) #'https://www.proxynova.com/proxy-server-list/anonymous-proxies/')\n",
    "df_pn = df_pn[0]\n",
    "df_pn = df_pn[~df_pn['Proxy IP'].isin(['(adsbygoogle = window.adsbygoogle || []).push({});'])]\n",
    "df_pn['IP'] = df_pn.apply(lambda x: str(x['Proxy IP']).replace('document.write(','').replace(');','').replace(\"'\",'') + \":\" + str(x['Proxy Port']), axis=1 )\n",
    "df_pn['IP'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver import ActionChains\n",
    "import time \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "url='https://proxyscrape.com/free-proxy-list'#'https://api.proxyscrape.com/v2/?request=share&protocol=socks4&timeout=400&country=all&simplified=true'\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_prefs = {}\n",
    "chrome_options.experimental_options[\"prefs\"] = chrome_prefs\n",
    "chrome_prefs[\"profile.default_content_settings\"] = {\"images\": 2}\n",
    "\n",
    "browser = webdriver.Chrome(options=chrome_options)\n",
    "browser.get(url)\n",
    "#get subpage urls \n",
    "browser.implicitly_wait(10)\n",
    "time.sleep(5)\n",
    "\n",
    "#close overlay if exists \n",
    "if len(browser.find_elements_by_xpath(\"//iframe[@class='HB-Takeover french-rose']\")) >0: #overlay active\n",
    "    print(\"clearing active overlay\")\n",
    "    time.sleep(15)\n",
    "    browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "    browser.switch_to_frame(browser.find_element_by_xpath(\"//iframe[@class='HB-Takeover french-rose']\"))\n",
    "    time.sleep(15)\n",
    "    click_stat=False\n",
    "    while click_stat ==False:\n",
    "        try:\n",
    "            browser.execute_script(\"arguments[0].click();\", browser.find_element_by_class_name(\"icon-close\"))\n",
    "            click_stat=True\n",
    "        except Exception as e: \n",
    "            print(\"error sleep 10, trying again:\", str(e))\n",
    "            time.sleep(10)\n",
    "    print(\"overlay cleared\")\n",
    "#get to proxy page\n",
    "browser.switch_to.default_content() #swap from iframe window\n",
    "# browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "browser.maximize_window()\n",
    "# https://stackoverflow.com/questions/40485157/how-to-move-range-input-using-selenium-in-python\n",
    "en =  browser.find_element_by_id(\"socks4timeoutslide\")\n",
    "move = ActionChains(browser)\n",
    "move.click_and_hold(en).move_by_offset(-95, 0).release().perform()\n",
    "button = browser.find_element_by_id(\"sharesocks4\") #browser.find_elements_by_class_name('downloadbtn')[1]\n",
    "button.click()\n",
    "#load new page\n",
    "browser.implicitly_wait(10)\n",
    "time.sleep(5)\n",
    "s_scrape = browser.find_element_by_css_selector(\"textarea\") \n",
    "proxylist=[] #stores IPs \n",
    "webpage=[] #\n",
    "proxy_list=[] #stores IP pages \n",
    "# count=0\n",
    "for IP in (s_scrape.text).splitlines(): #add to list \n",
    "        proxylist.append(IP)\n",
    "        webpage.append('proxy-list.download')\n",
    "print(\"done scraping\")\n",
    "#now write to df \n",
    "df_proxy_list = pd.DataFrame(\n",
    "    np.column_stack([proxylist, webpage]), \n",
    "    columns=['proxy','webpage'])\n",
    "print(df_proxy_list.head())\n",
    "browser.quit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver import ActionChains\n",
    "import time \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "url='https://www.proxy-list.download/SOCKS4'\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_prefs = {}\n",
    "chrome_options.experimental_options[\"prefs\"] = chrome_prefs\n",
    "chrome_prefs[\"profile.default_content_settings\"] = {\"images\": 2}\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")\n",
    "browser = webdriver.Chrome(options=chrome_options)\n",
    "browser.get(url)\n",
    "#get subpage urls \n",
    "browser.implicitly_wait(10)\n",
    "time.sleep(5)\n",
    "s_scrape = browser.find_element_by_id(\"downloadbtn\") #browser.find_element_by_css_selector(\"textarea\") \n",
    "s_scrape.click() #download as txt file \n",
    "df_proxy_list = pd.read_csv('Proxy List.txt',sep=\"\\t\", names=['proxy']) #import to df \n",
    "df_proxy_list['webage']='proxy-list.download'\n",
    "\n",
    "browser.quit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "url='https://www.proxynova.com/proxy-server-list/anonymous-proxies/'\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")\n",
    "browser = webdriver.Chrome(executable_path=r\"C:\\Users\\chapo\\Downloads\\chromedriver.exe\", chrome_options=chrome_options)\n",
    "browser.get(url)\n",
    "\n",
    "table = browser.find_element_by_id(\"tbl_proxy_list\")\n",
    "# df_pn = pd.read_html(browser.text) #'https://www.proxynova.com/proxy-server-list/anonymous-proxies/')\n",
    "proxylist=[]\n",
    "webpage=[]\n",
    "for a in table.text.splitlines():\n",
    "    if '.' in a: \n",
    "        a_split= a.split(\" \")\n",
    "        proxylist.append(a_split[0] + \":\" + a_split[1])\n",
    "        webpage.append('proxynova.com')\n",
    "print(\"done scraping\")\n",
    "\n",
    "#now write to df \n",
    "df_proxy_list = pd.DataFrame(\n",
    "    np.column_stack([proxylist, webpage]), \n",
    "    columns=['proxy','webpage'])\n",
    "print(df_proxy_list.head())\n",
    "\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver import ActionChains\n",
    "import time \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "url='https://www.proxynova.com/proxy-server-list/anonymous-proxies/'\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_prefs = {}\n",
    "chrome_options.experimental_options[\"prefs\"] = chrome_prefs\n",
    "chrome_prefs[\"profile.default_content_settings\"] = {\"images\": 2}\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")\n",
    "browser = webdriver.Chrome(options=chrome_options)\n",
    "browser.get(url)\n",
    "\n",
    "table = browser.find_element_by_id(\"tbl_proxy_list\")\n",
    "# df_pn = pd.read_html(browser.text) #'https://www.proxynova.com/proxy-server-list/anonymous-proxies/')\n",
    "proxylist=[]\n",
    "webpage=[]\n",
    "for a in table.text.splitlines():\n",
    "    if '.' in a: \n",
    "        a_split= a.split(\" \")\n",
    "        proxylist.append(a_split[0] + \":\" + a_split[1])\n",
    "        webpage.append('proxynova.com')\n",
    "\n",
    "print(\"done scraping\")\n",
    "#now write to df \n",
    "df_proxy_list = pd.DataFrame(\n",
    "    np.column_stack([proxylist, webpage]), \n",
    "    columns=['proxy','webpage'])\n",
    "\n",
    "print(df_proxy_list.head())\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxylist=[] #stores IPs \n",
    "webpage=[] #\n",
    "proxy_list=[] #stores IP pages \n",
    "# count=0\n",
    "for IP in (s_scrape.text).splitlines(): #add to list \n",
    "        proxylist.append(IP)\n",
    "        webpage.append('proxyscrape.com')\n",
    "print(\"done scraping\")\n",
    "#now write to df \n",
    "df_proxy_list = pd.DataFrame(\n",
    "    np.column_stack([proxylist, webpage]), \n",
    "    columns=['proxy','webpage'])\n",
    "print(df_proxy_list.head())\n",
    "# browser.quit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#close overlay if exists \n",
    "if len(browser.find_elements_by_xpath(\"//iframe[@class='HB-Takeover french-rose']\")) >0: #overlay active\n",
    "    print(\"clearing active overlay\")\n",
    "    time.sleep(15)\n",
    "    browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "    browser.switch_to_frame(browser.find_element_by_xpath(\"//iframe[@class='HB-Takeover french-rose']\"))\n",
    "    time.sleep(15)\n",
    "    click_stat=False\n",
    "    while click_stat ==False:\n",
    "        try:\n",
    "            browser.execute_script(\"arguments[0].click();\", browser.find_element_by_class_name(\"icon-close\"))\n",
    "            click_stat=True\n",
    "        except Exception as e: \n",
    "            print(\"error sleep 10, trying again:\", str(e))\n",
    "            time.sleep(10)\n",
    "    print(\"overlay cleared\")\n",
    "#get to proxy page\n",
    "browser.switch_to.default_content() #swap from iframe window\n",
    "# browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "browser.maximize_window()\n",
    "# https://stackoverflow.com/questions/40485157/how-to-move-range-input-using-selenium-in-python\n",
    "en =  browser.find_element_by_id(\"socks4timeoutslide\")\n",
    "move = ActionChains(browser)\n",
    "move.click_and_hold(en).move_by_offset(-95, 0).release().perform()\n",
    "button = browser.find_element_by_id(\"sharesocks4\") #browser.find_elements_by_class_name('downloadbtn')[1]\n",
    "button.click()\n",
    "#load new page\n",
    "browser.implicitly_wait(10)\n",
    "time.sleep(5)\n",
    "s_scrape = browser.find_element_by_css_selector(\"textarea\") \n",
    "proxylist=[] #stores IPs \n",
    "webpage=[] #\n",
    "proxy_list=[] #stores IP pages \n",
    "# count=0\n",
    "for IP in (s_scrape.text).splitlines(): #add to list \n",
    "        proxylist.append(IP)\n",
    "        webpage.append(proxyscrape.com)\n",
    "print(\"done scraping\")\n",
    "#now write to df \n",
    "df_proxy_list = pd.DataFrame(\n",
    "    np.column_stack([proxylist, webpage]), \n",
    "    columns=['proxy','webpage'])\n",
    "print(df_proxy_list.head())\n",
    "browser.quit() \n",
    "status=True\n",
    "    # except Exception as e:\n",
    "    #     print(\"error:\", str(e))\n",
    "    #     status=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver import ActionChains\n",
    "import time \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "url='https://proxyscrape.com/free-proxy-list'#'https://api.proxyscrape.com/v2/?request=share&protocol=socks4&timeout=400&country=all&simplified=true'\n",
    "status=False\n",
    "# while status==False:\n",
    "    # try: \n",
    "browser = webdriver.Chrome(executable_path=r\"C:\\Users\\chapo\\Downloads\\chromedriver.exe\")\n",
    "browser.get(url)\n",
    "#get subpage urls \n",
    "browser.implicitly_wait(10)\n",
    "time.sleep(5)\n",
    "\n",
    "#close overlay if exists \n",
    "if len(browser.find_elements_by_xpath(\"//iframe[@class='HB-Takeover french-rose']\")) >0: #overlay active\n",
    "    print(\"clearing active overlay\")\n",
    "    time.sleep(15)\n",
    "    browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "    browser.switch_to_frame(browser.find_element_by_xpath(\"//iframe[@class='HB-Takeover french-rose']\"))\n",
    "    time.sleep(15)\n",
    "    click_stat=False\n",
    "    while click_stat ==False:\n",
    "        try:\n",
    "            browser.execute_script(\"arguments[0].click();\", browser.find_element_by_class_name(\"icon-close\"))\n",
    "            click_stat=True\n",
    "        except Exception as e: \n",
    "            print(\"error sleep 10, trying again:\", str(e))\n",
    "            time.sleep(10)\n",
    "    print(\"overlay cleared\")\n",
    "#get to proxy page\n",
    "browser.switch_to.default_content() #swap from iframe window\n",
    "# browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "browser.maximize_window()\n",
    "# https://stackoverflow.com/questions/40485157/how-to-move-range-input-using-selenium-in-python\n",
    "en =  browser.find_element_by_id(\"socks4timeoutslide\")\n",
    "move = ActionChains(browser)\n",
    "move.click_and_hold(en).move_by_offset(-95, 0).release().perform()\n",
    "button = browser.find_element_by_id(\"sharesocks4\") #browser.find_elements_by_class_name('downloadbtn')[1]\n",
    "button.click()\n",
    "#load new page\n",
    "browser.implicitly_wait(10)\n",
    "time.sleep(5)\n",
    "s_scrape = browser.find_element_by_css_selector(\"textarea\") \n",
    "proxylist=[] #stores IPs \n",
    "webpage=[] #\n",
    "proxy_list=[] #stores IP pages \n",
    "# count=0\n",
    "for IP in (s_scrape.text).splitlines(): #add to list \n",
    "        proxylist.append(IP)\n",
    "        webpage.append(proxyscrape.com)\n",
    "print(\"done scraping\")\n",
    "#now write to df \n",
    "df_proxy_list = pd.DataFrame(\n",
    "    np.column_stack([proxylist, webpage]), \n",
    "    columns=['proxy','webpage'])\n",
    "print(df_proxy_list.head())\n",
    "browser.quit() \n",
    "status=True\n",
    "    # except Exception as e:\n",
    "    #     print(\"error:\", str(e))\n",
    "    #     status=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url='http://free-proxy.cz/en/proxylist/country/all/http/ping/level2'\n",
    "# df_res = pd.DataFrame()\n",
    "# _page = 1\n",
    "# headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "# session = requests.Session()\n",
    "# retry = Retry(connect=5, backoff_factor=0.5)\n",
    "# adapter = HTTPAdapter(max_retries=retry)\n",
    "# session.mount('http://', adapter)\n",
    "# session.mount('https://', adapter)\n",
    "# while _page <= 5: \n",
    "#     proxy_table = pd.DataFrame()\n",
    "#     time.sleep(4)\n",
    "#     print(url + '/' + str(_page))\n",
    "#     # session.get(baseurl)\n",
    "#     if _page ==1: response = session.get(url, headers=headers)\n",
    "#     else: response = session.get(url + '/' + str(_page), headers=headers)\n",
    "#     # parser = html.fromstring(response.text)\n",
    "#     IP=[]\n",
    "#     result=False\n",
    "#     root = lxml.etree.HTML(response.text)\n",
    "#     toner_id='proxy_list'\n",
    "#     results = root.xpath(\"//table[@id =@id = '%s']\" % toner_id)\n",
    "#     #parse table needed\n",
    "#     pd_tables = [pd.read_html(lxml.etree.tostring(table,method='html')) for table in results]\n",
    "#     proxy_table = pd_tables[0][0]\n",
    "    \n",
    "#     proxy_table = proxy_table[~proxy_table['IP address'].isin(['(adsbygoogle = window.adsbygoogle || []).push({});'])]\n",
    "#     proxy_table['IP address']=proxy_table['IP address'].apply(lambda x: str( str( base64.b64decode( str(x).replace('document.write(Base64.decode(\"','').replace('\"))','') ) ) ).replace('b','').replace(\"'\",'') ) \n",
    "#     proxy_table['IP_port']=proxy_table.apply(lambda x: x['IP address'] + ':' + x['Port'], axis=1)\n",
    "#     proxy_table['page_id']=_page\n",
    "#     print(\"page table len:\", str(len(proxy_table)))\n",
    "#     df_res = df_res.append(proxy_table)\n",
    "#     # print(\"done with:\", _page)\n",
    "#     _page += 1\n",
    "# session.close() \n",
    "# df_res.to_csv(r'C:\\Users\\chapo\\Downloads\\aa\\test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url='http://www.freeproxylists.net/?c=&pt=&pr=&a%5B%5D=1&a%5B%5D=2&u=50'\n",
    "# # df_fp = pd.read_html('http://free-proxy.cz/en/proxylist/country/all/http/ping/level2')\n",
    "# # https://www.proxynova.com/proxy-server-list/anonymous-proxies/\n",
    "# df_res = pd.DataFrame()\n",
    "# _page = 1\n",
    "# headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "# session = requests.Session()\n",
    "# retry = Retry(connect=5, backoff_factor=0.5)\n",
    "# adapter = HTTPAdapter(max_retries=retry)\n",
    "# session.mount('http://', adapter)\n",
    "# session.mount('https://', adapter)\n",
    "\n",
    "# # proxy_table = pd.DataFrame()\n",
    "# # time.sleep(4)\n",
    "# response = session.get(url, headers=headers, proxies={'http:' : 'http://139.224.47.77:8080', 'https:' : 'http://139.224.47.77:8080'})\n",
    "# # # parser = html.fromstring(response.text)\n",
    "# # IP=[]\n",
    "# # result=False\n",
    "# # root = lxml.etree.HTML(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url='http://free-proxy.cz/en/proxylist/country/all/http/ping/level2'\n",
    "# df_res = pd.DataFrame()\n",
    "# _page = 1\n",
    "# # headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "# session = requests.Session()\n",
    "# retry = Retry(connect=5, backoff_factor=0.5)\n",
    "# adapter = HTTPAdapter(max_retries=retry)\n",
    "# # session.mount('http://', adapter)\n",
    "# session.mount('https://', adapter)\n",
    "\n",
    "# proxy_table = pd.DataFrame()\n",
    "# time.sleep(4)\n",
    "# print(url + '/' + str(_page))\n",
    "# # session.get(baseurl)\n",
    "# if _page ==1: response = session.get(url, headers=headers)\n",
    "# else: response = session.get(url + '/' + str(_page), headers=headers)\n",
    "# # parser = html.fromstring(response.text)\n",
    "# IP=[]\n",
    "# result=False\n",
    "# root = lxml.etree.HTML(response.text)\n",
    "\n",
    "\n",
    "# toner_id='proxy_list'\n",
    "# results = root.xpath(\"//table[@id =@id = '%s']\" % toner_id)\n",
    "# #parse table needed\n",
    "# # pd_tables = [pd.read_html(lxml.etree.tostring(table,method='html')) for table in results]\n",
    "# # proxy_table = pd_tables[0][0]"
   ]
  }
 ]
}