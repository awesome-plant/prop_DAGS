{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNU https://spys.one/en/https-ssl-proxy/\n",
    "# DNU http://free-proxy.cz/en/proxylist/country/all/http/ping/level2/2\n",
    "# DNU http://www.freeproxylists.net/?c=&pt=&pr=&a%5B%5D=1&a%5B%5D=2&u=50\n",
    "# -DONE https://openproxy.space/list\n",
    "# -DONE https://proxyscrape.com/free-proxy-list\n",
    "# -DONE https://www.proxy-list.download/\n",
    "# DONE https://www.proxynova.com/proxy-server-list/anonymous-proxies/\n",
    "# https://www.proxyscan.io/\n",
    "# https://free-proxy-list.net/anonymous-proxy.html\n",
    "# https://www.hidemyass-freeproxy.com/ ? \n",
    "# https://nl.hideproxy.me/ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "inserting into tables: sc_source_header, sc_source_file\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'create_engine' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-6a16b73cc78b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[0mXML_H_Dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'h_fileid'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXML_H_Dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'h_fileid'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"inserting into tables: sc_source_header, sc_source_file\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m \u001b[0mengine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'postgresql://postgres:root@172.22.114.65:5432/scrape_db'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m XML_H_Dataset.to_sql(\n\u001b[0;32m     71\u001b[0m     \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sc_source_header'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'create_engine' is not defined"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.proxy import Proxy, ProxyType\n",
    "\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "import datetime \n",
    "import numpy as np\n",
    "\n",
    "prox = Proxy()\n",
    "prox.proxy_type = ProxyType.MANUAL\n",
    "prox.http_proxy = proxy\n",
    "prox.socks_proxy = proxy\n",
    "prox.ssl_proxy = proxy\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")\n",
    "browser = webdriver.Chrome(executable_path=r\"C:\\Users\\chapo\\Downloads\\chromedriver.exe\", chrome_options=chrome_options)\n",
    "\n",
    "site_url='https://www.realestate.com.au/xml-sitemap/'\n",
    "browser.get(site_url)\n",
    "\n",
    "root = etree.fromstring(browser.page_source)\n",
    "body=root.xpath('//ns:Contents',namespaces={'ns':\"http://s3.amazonaws.com/doc/2006-03-01/\"})\n",
    "browser.quit()\n",
    "\n",
    "#header link ref\n",
    "XML_H_Dataset=pd.DataFrame({ \n",
    "    'external_ip': str(proxy)\n",
    "    , 'h_filename': str(\"XML_scrape_\" + (datetime.datetime.now()).strftime('%Y-%m-%d'))\n",
    "    , 'scrape_dt': (datetime.datetime.now())\n",
    "    },index=[0]\n",
    ")\n",
    "\n",
    "#now we read/parse the xml\n",
    "l_suffix=[]\n",
    "l_filename=[]\n",
    "l_filetype=[]\n",
    "l_lastmod=[]\n",
    "l_filesize_kb=[]\n",
    "l_storageclass=[]\n",
    "for element in body:\n",
    "    _Type=''\n",
    "    if 'buy' in (element[0].text).lower(): \n",
    "        _Type='buy'\n",
    "    elif 'sold' in (element[0].text).lower(): \n",
    "        _Type='sold'\n",
    "    elif 'rent' in (element[0].text).lower(): \n",
    "        _Type='rent'\n",
    "    l_suffix.append(str(element[0].text).split('-')[0])\n",
    "    l_filename.append(str(element[0].text))\n",
    "    l_filetype.append(_Type)\n",
    "    l_lastmod.append(datetime.datetime.strptime((element[1].text).replace('Z','+0000'), '%Y-%m-%dT%H:%M:%S.%f%z') )\n",
    "    l_filesize_kb.append(round(int(element[3].text) / 1000))\n",
    "    l_storageclass.append(str(element[4].text))\n",
    "\n",
    "XML_S_Dataset = pd.DataFrame(\n",
    "    np.column_stack([l_suffix, l_filename, l_filetype, l_lastmod, l_filesize_kb, l_storageclass]), \n",
    "    columns=['suffix','s_filename', 'filetype', 'lastmod', 's_filesize_kb', 'storageclass'])\n",
    "XML_S_Dataset['lastmod']=pd.to_datetime(XML_S_Dataset['lastmod'])\n",
    "XML_S_Dataset['s_filesize_kb']=pd.to_numeric(XML_S_Dataset['s_filesize_kb'])\n",
    "\n",
    "fileID=getFileID()\n",
    "XML_S_Dataset['h_fileid']=fileID\n",
    "XML_H_Dataset['h_fileid']=fileID\n",
    "XML_S_Dataset['h_fileid']=pd.to_numeric(XML_S_Dataset['h_fileid'])\n",
    "XML_H_Dataset['h_fileid']=pd.to_numeric(XML_H_Dataset['h_fileid'])\n",
    "print(\"inserting into tables: sc_source_header, sc_source_file\")\n",
    "\n",
    "db_import.insertData(ps_user=\"postgres\", ps_pass=\"root\", ps_host=\"172.22.114.65\", ps_port=\"5432\", ps_db=\"scrape_db\", table='sc_source_header', df_insert=XML_H_Dataset)\n",
    "db_import.insertData(ps_user=\"postgres\", ps_pass=\"root\", ps_host=\"172.22.114.65\", ps_port=\"5432\", ps_db=\"scrape_db\", table='sc_source_file', df_insert=XML_S_Dataset)\n",
    "print(\"inserts completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "def getFileID():\n",
    "# Connect to an existing database\n",
    "    connection = psycopg2.connect(user=\"postgres\",password=\"root\",host=\"172.22.114.65\",port=\"5432\",database=\"scrape_db\")\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"SELECT coalesce(max(H_FILEID), 0) + 1 as h_fileid from sc_land.SC_SOURCE_HEADER\")\n",
    "    h_fileid = cursor.fetchone() #next iteration of file ID \n",
    "    return h_fileid[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time \n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import datetime \n",
    "from selenium.webdriver.common.proxy import Proxy, ProxyType\n",
    "\n",
    "prox = Proxy()\n",
    "prox.proxy_type = ProxyType.MANUAL\n",
    "prox.http_proxy = proxy\n",
    "prox.socks_proxy = proxy\n",
    "prox.ssl_proxy = proxy\n",
    "url='http://ident.me/'\n",
    "\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")\n",
    "browser = webdriver.Chrome(executable_path=r\"C:\\Users\\chapo\\Downloads\\chromedriver.exe\", chrome_options=chrome_options)\n",
    "\n",
    "try:\n",
    "    browser.set_page_load_timeout(3)\n",
    "    browser.get(url)\n",
    "    _newIP = browser.find_element_by_tag_name(\"body\").text\n",
    "    if my_ip !=_newIP: #IP masked\n",
    "        try: \n",
    "            site_url='https://www.realestate.com.au/'\n",
    "            browser.get(site_url)\n",
    "            # r = requests.get(site_url, headers=headers, proxies=proxies, timeout=timeout)\n",
    "            status=True\n",
    "        except Exception as e:\n",
    "            error = url + '-' + str(e) \n",
    "    else: error = url + '-no IP mask -' + _newIP\n",
    "except Exception as e: \n",
    "    error = url + '-' + str(e) \n",
    "\n",
    "browser.quit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy='78.9.110.94:1080'\n",
    "timeout=3\n",
    "my_ip='124.170.26.96'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'http://ident.me/-no IP mask -124.170.26.96'"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "#docker script\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.common.proxy import Proxy, ProxyType\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_prefs = {}\n",
    "chrome_options.experimental_options[\"prefs\"] = chrome_prefs\n",
    "chrome_prefs[\"profile.default_content_settings\"] = {\"images\": 2}\n",
    "browser = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "prox = Proxy()\n",
    "prox.proxy_type = ProxyType.MANUAL\n",
    "prox.http_proxy = proxy\n",
    "prox.socks_proxy = proxy\n",
    "prox.ssl_proxy = proxy\n",
    "url='http://ident.me/'\n",
    "\n",
    "try:\n",
    "    browser.set_page_load_timeout(3)\n",
    "    browser.get(url)\n",
    "    _newIP = browser.find_element_by_tag_name(\"body\").text\n",
    "    if my_ip !=_newIP: #IP masked\n",
    "        try: \n",
    "            site_url='https://www.realestate.com.au/'\n",
    "            browser.get(site_url)\n",
    "            # r = requests.get(site_url, headers=headers, proxies=proxies, timeout=timeout)\n",
    "            status=True\n",
    "        except Exception as e:\n",
    "            error = url + '-' + str(e) \n",
    "    else: error = url + '-no IP mask -' + _newIP\n",
    "except Exception as e: \n",
    "    error = url + '-' + str(e) \n",
    "\n",
    "browser.quit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docker script\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver import ActionChains\n",
    "import time \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime  \n",
    "import lxml\n",
    "\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "url='https://www.proxyscan.io/'\n",
    "ping=300\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_prefs = {}\n",
    "chrome_options.experimental_options[\"prefs\"] = chrome_prefs\n",
    "chrome_prefs[\"profile.default_content_settings\"] = {\"images\": 2}\n",
    "browser = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "browser.get(url)\n",
    "#click buttons \n",
    "browser.find_element_by_id(\"pingText\").send_keys(str(ping))\n",
    "ActionChains(browser).move_to_element(browser.find_element_by_id(\"anonymous\")).click().perform()\n",
    "ActionChains(browser).move_to_element(browser.find_element_by_id(\"elite\")).click().perform()\n",
    "#scroll to bottom of page\n",
    "sc_stat=False\n",
    "ph=0 #page height\n",
    "while sc_stat==False:\n",
    "    if browser.execute_script(\"return window.pageYOffset\") <= browser.execute_script(\"return document.documentElement.scrollHeight\"): \n",
    "        ph = browser.execute_script(\"return window.pageYOffset\")\n",
    "        browser.execute_script(\"window.scrollTo(0,document.documentElement.scrollHeight)\")\n",
    "        time.sleep(1)\n",
    "        if ph == browser.execute_script(\"return window.pageYOffset\"): \n",
    "            sc_stat=True \n",
    "\n",
    "#get table \n",
    "table_MN = pd.read_html(browser.page_source)\n",
    "filtered = table_MN[0][table_MN[0]['Anonymity']!='Transparent']\n",
    "df_proxy_list=pd.DataFrame()\n",
    "df_proxy_list['proxy'] = filtered['Ip Address'].astype(str) + \":\" + filtered['Port'].astype(str)\n",
    "df_proxy_list['website']='proxyscan'\n",
    "df_proxy_list['scrape_dt']=datetime.datetime.now()\n",
    "\n",
    "browser.quit() \n",
    "print(\"done scraping, now writing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ProxyError",
     "evalue": "HTTPConnectionPool(host='78.9.110.94', port=1080): Max retries exceeded with url: http://ident.me/ (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response')))",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteDisconnected\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    671\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 672\u001b[1;33m                 \u001b[0mchunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    673\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    420\u001b[0m                     \u001b[1;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m                     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    415\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 416\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    417\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1343\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1344\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1345\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    274\u001b[0m             \u001b[1;31m# sending a valid response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m             raise RemoteDisconnected(\"Remote end closed connection without\"\n\u001b[0m\u001b[0;32m    276\u001b[0m                                      \" response\")\n",
      "\u001b[1;31mRemoteDisconnected\u001b[0m: Remote end closed connection without response",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m                 )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    719\u001b[0m             retries = retries.increment(\n\u001b[1;32m--> 720\u001b[1;33m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    721\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\util\\retry.py\u001b[0m in \u001b[0;36mincrement\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    435\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 436\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='78.9.110.94', port=1080): Max retries exceeded with url: http://ident.me/ (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response')))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProxyError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-482f34022fa4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# session.headers.update(headers)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 543\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GET'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    544\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    545\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    528\u001b[0m         }\n\u001b[0;32m    529\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 643\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ProxyError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mProxyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    511\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_SSLError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mProxyError\u001b[0m: HTTPConnectionPool(host='78.9.110.94', port=1080): Max retries exceeded with url: http://ident.me/ (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response')))"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "# from requests.adapters import HTTPAdapter\n",
    "# from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "url='http://ident.me/'\n",
    "proxy='78.9.110.94:1080'\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "    \n",
    "proxies= { 'http': 'http://' + proxy, 'https': 'https://' + proxy } \n",
    "with requests.Session() as session:\n",
    "    # retry = requests.packages.urllib3.util.retry.Retry(connect=1, backoff_factor=0.5)\n",
    "    # adapter = requests.adapters.HTTPAdapter(max_retries=retry)\n",
    "    # session.mount('http://', adapter)\n",
    "    # session.mount('https://', adapter)\n",
    "    # session.headers.update(headers)\n",
    "    session.timeout=3\n",
    "    session.get(url, proxies=proxies, timeout=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time \n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import datetime \n",
    "\n",
    "ping=300\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "url='https://www.proxyscan.io/'\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")\n",
    "browser = webdriver.Chrome(executable_path=r\"C:\\Users\\chapo\\Downloads\\chromedriver.exe\", chrome_options=chrome_options)\n",
    "browser.get(url)\n",
    "#click buttons \n",
    "browser.find_element_by_id(\"pingText\").send_keys(str(ping))\n",
    "ActionChains(browser).move_to_element(browser.find_element_by_id(\"anonymous\")).click().perform()\n",
    "ActionChains(browser).move_to_element(browser.find_element_by_id(\"elite\")).click().perform()\n",
    "#scroll to bottom of page\n",
    "sc_stat=False\n",
    "ph=0 #page height\n",
    "while sc_stat==False:\n",
    "    if browser.execute_script(\"return window.pageYOffset\") <= browser.execute_script(\"return document.documentElement.scrollHeight\"): \n",
    "        ph = browser.execute_script(\"return window.pageYOffset\")\n",
    "        browser.execute_script(\"window.scrollTo(0,document.documentElement.scrollHeight)\")\n",
    "        time.sleep(1)\n",
    "        if ph == browser.execute_script(\"return window.pageYOffset\"): \n",
    "            sc_stat=True \n",
    "#get table \n",
    "table_MN = pd.read_html(browser.page_source)\n",
    "filtered = table_MN[0][table_MN[0]['Anonymity']!='Transparent']\n",
    "df_proxy_list=pd.DataFrame()\n",
    "df_proxy_list['proxy'] = filtered['Ip Address'].astype(str) + \":\" + filtered['Port'].astype(str)\n",
    "df_proxy_list['website']='proxyscan'\n",
    "df_proxy_list['scrape_dt']=datetime.datetime.now()\n",
    "\n",
    "browser.quit() \n",
    "print(\"done scraping, now writing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proxy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "proxy='86.125.112.230:30897'\n",
    "timeout=3\n",
    "my_ip='124.170.26.96'\n",
    "status=False\n",
    "error=None\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "proxies= { 'http': 'http://' + proxy, 'https:': 'https://' + proxy}#, 'https': 'https://' + proxy } \n",
    "url='http://ident.me/'\n",
    "try:\n",
    "    r = requests.get(url, headers=headers, proxies=proxies, timeout=timeout)\n",
    "    _newIP = r.text\n",
    "    # if my_ip !=_newIP: #IP masked\n",
    "    #     try: \n",
    "    #         site_url='https://www.realestate.com.au/'\n",
    "    #         r = requests.get(site_url, headers=headers, proxies=proxies, timeout=timeout)\n",
    "    #         status=True\n",
    "    #     except Exception as e:\n",
    "    #         error = url + '-' + str(e) \n",
    "    # else: error = url + '-no IP mask -' + _newIP\n",
    "except Exception as e: \n",
    "    error = url + '-' + str(e) \n",
    "# print(\"proxy:\", proxy, '-result:', result, '-error:', error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_newIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "from psycopg2 import Error\n",
    "import csv\n",
    "from io import StringIO\n",
    "def getProxies(ps_user, ps_pass, ps_host, ps_port, ps_db, sql_start, sql_size):\n",
    "    import pandas as pd\n",
    "    #queries db and returns list of all proxies within paramaters \n",
    "    with psycopg2.connect(user=ps_user,password=ps_pass,host=ps_host,port=ps_port,database=ps_db) as conn:\n",
    "        check_proxy_list=pd.read_sql_query(\"SELECT proxy FROM sc_land.sc_proxy_raw order by table_id limit \" + str(sql_size) + \" offset \" + str(sql_start), conn)\n",
    "        print(\"got row list, start:\", str(sql_start), 'length:', str(sql_size))\n",
    "    return check_proxy_list \n",
    "def testProxy(proxy, timeout, my_ip, **kwargs):\n",
    "    # def here returns proxy, confirmed with different whatismyip return \n",
    "    #return true when dif\n",
    "    status=False\n",
    "    error=None\n",
    "    headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "    proxies= { 'http': 'http://' + proxy, 'https': 'https://' + proxy } \n",
    "    url='https://ident.me/'\n",
    "    try:\n",
    "        r = requests.get(url, headers=headers, proxies=proxies, timeout=timeout)\n",
    "        _newIP = r.text\n",
    "        if my_ip !=_newIP: #IP masked\n",
    "            try: \n",
    "                site_url='https://www.realestate.com.au/'\n",
    "                r = requests.get(site_url, headers=headers, proxies=proxies, timeout=timeout)\n",
    "                status=True\n",
    "            except Exception as e:\n",
    "                error = url + '-' + str(e) \n",
    "        else: error = url + '-no IP mask'\n",
    "    except Exception as e: \n",
    "        error = url + '-' + str(e) \n",
    "    # print(\"proxy:\", proxy, '-result:', status, '-error:', error)\n",
    "    return np.array([status, error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_proxy_list = getProxies(\n",
    "        ps_user=\"postgres\"\n",
    "        , ps_pass=\"root\"\n",
    "        , ps_host=\"172.22.114.65\"\n",
    "        , ps_port=\"5432\"\n",
    "        , ps_db=\"scrape_db\"\n",
    "        , sql_start=0\n",
    "        , sql_size=20\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "myIP='124.170.26.96'\n",
    "print(\"myIP is:\", myIP)\n",
    "\n",
    "l_proxy=[]\n",
    "l_status=[]\n",
    "l_error=[]\n",
    "for index, row in check_proxy_list.iterrows(): #dont judge me \n",
    "    print(row['proxy'])\n",
    "    status= error=''\n",
    "    status, error = testProxy(proxy=row['proxy'],timeout=3, my_ip=myIP)\n",
    "    l_proxy.append(row['proxy'])\n",
    "    l_status.append(status)\n",
    "    l_error.append(error)\n",
    "\n",
    "df_proxy_list = pd.DataFrame(\n",
    "        np.column_stack([l_proxy, l_status,l_error]), \n",
    "        columns=['proxy','status','error'])\n",
    "\n",
    "with pd.option_context('display.max_rows', len(check_proxy_list), 'display.max_columns', None):  # more options can be specified also\n",
    "    print(\"proxy list cols:\", df_proxy_list.dtypes)\n",
    "    print(df_proxy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_proxy_list=df_proxy_list\n",
    "check_proxy_list[check_proxy_list['error'].notnull()].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_proxy_list[check_proxy_list['error'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateProxies(ps_user, ps_pass, ps_host, ps_port, ps_db, proxy_list, value):\n",
    "    #updates proxy as broken.\n",
    "    with psycopg2.connect(user=ps_user,password=ps_pass,host=ps_host,port=ps_port,database=ps_db) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            proxy_list.apply(lambda x: \n",
    "                cur.execute(\"\"\"\n",
    "                    update sc_land.sc_proxy_raw \n",
    "                    set status = %(value)s \n",
    "                    ,error = %(error)s \n",
    "                    where proxy = %(proxy)s\"\"\"\n",
    "                    , { 'proxy': x['proxy'], 'value': value, 'error': x['error'] }\n",
    "                    ), axis=1 \n",
    "                    \n",
    "            )\n",
    "            conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updateProxies(\n",
    "        ps_user=\"postgres\"\n",
    "        , ps_pass=\"root\"\n",
    "        , ps_host=\"172.22.114.65\"\n",
    "        , ps_port=\"5432\"\n",
    "        , ps_db=\"scrape_db\"\n",
    "        , proxy_list = check_proxy_list[check_proxy_list['error'].notnull()]\n",
    "        , value='broken'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time \n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "url='https://ident.me/'\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")\n",
    "browser = webdriver.Chrome(executable_path=r\"C:\\Users\\chapo\\Downloads\\chromedriver.exe\", chrome_options=chrome_options)\n",
    "browser.get(url)\n",
    "\n",
    "IP = re.sub('<[^<]+?>', '', browser.page_source)\n",
    "print(IP)\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "IP = re.sub('<[^<]+?>', '', browser.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "timeout =3\n",
    "proxy = '58.147.170.132:10801'\n",
    "result=''\n",
    "\n",
    "\n",
    "def updateProxies(ps_user, ps_pass, ps_host, ps_port, ps_db, proxy_list, value):\n",
    "    #updates proxy as broken.\n",
    "    with psycopg2.connect(user=ps_user,password=ps_pass,host=ps_host,port=ps_port,database=ps_db) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            proxy_list.apply(lambda x: \n",
    "                cur.execute(\"\"\"\n",
    "                    update sc_land.sc_proxy_raw \n",
    "                    set status = %(value)s \n",
    "                    ,error = %(error)s \n",
    "                    where proxy = %(proxy)s\"\"\"\n",
    "                    , { 'proxy': x['proxy'], 'value': value, 'error': x['error'] }\n",
    "                    ), axis=1 \n",
    "                    \n",
    "            )\n",
    "            conn.commit()\n",
    "_newIP=_actualIP\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "proxies= { 'http': 'http://' + proxy, 'https': 'https://' + proxy } \n",
    "url='https://ident.me/'\n",
    "q=requests.get(url, headers=headers)\n",
    "\n",
    "try:\n",
    "    r = requests.get(url, headers=headers, proxies=proxies, timeout=timeout)\n",
    "    _newIP = r.text\n",
    "    if _actualIP !=_newIP: #IP masked\n",
    "        print(\"worked\")\n",
    "        try: \n",
    "            \n",
    "            site_url='https://www.realestate.com.au/'\n",
    "            r = requests.get(site_url, headers=headers, proxies=proxies, timeout=timeout)\n",
    "            # print(\"IP:\", proxy, \"-capable of scraping:\",site_url)\n",
    "            result=True\n",
    "        except Exception as e:\n",
    "            result='error: ' + str(e)\n",
    "    else: print(\"wrong IP:\", r.text)\n",
    "except Exception as e:\n",
    "    print(url, e) \n",
    "    # pass\n",
    "# if result==True: print(\"IP:\", proxy, \"-capable of scraping:\",site_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime \n",
    "import time \n",
    "import random\n",
    "baseurl='https://www.realestate.com.au/xml-sitemap/'\n",
    "        # , 'RootDir': '/opt/airflow/logs/XML_save_folder' \n",
    "PageSaveFolder=r'D:\\shared_folder\\airflow_logs\\XML_save_folder\\gz_files'#'/opt/airflow/logs/XML_save_folder/raw_sitemap/'\n",
    "Scrapewait=5\n",
    "# def ScrapeURL(baseurl, PageSaveFolder, Scrapewait, **kwargs):  \n",
    "XMLsaveFile=\"XML_scrape_\" + (datetime.datetime.now()).strftime('%Y-%m-%d')\n",
    "time.sleep(random.randint(1,10))\n",
    "# ua = UserAgent()\n",
    "#headers = {'User-Agent':str(ua.random)}\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "url='https://free-proxy-list.net/anonymous-proxy.html'\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")\n",
    "browser = webdriver.Chrome(executable_path=r\"C:\\Users\\chapo\\Downloads\\chromedriver.exe\", chrome_options=chrome_options)\n",
    "browser.get(url)\n",
    "\n",
    "browser.find_element_by_xpath(\"//select[@name='proxylisttable_length']/option[text()='80']\").click()\n",
    "\n",
    "table_MN = pd.read_html(browser.page_source)\n",
    "browser.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_MN[0][table_MN[0]['IP Address'].notnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docker script\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver import ActionChains\n",
    "import time \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_prefs = {}\n",
    "chrome_options.experimental_options[\"prefs\"] = chrome_prefs\n",
    "chrome_prefs[\"profile.default_content_settings\"] = {\"images\": 2}\n",
    "\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "url='https://openproxy.space/list'\n",
    "browser = webdriver.Chrome(options=chrome_options)\n",
    "browser.get(url)\n",
    "browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "#get subpage urls \n",
    "browser.implicitly_wait(10)\n",
    "time.sleep(5)\n",
    "ListlinkerHref = browser.find_elements_by_xpath(\"//*[@href]\")\n",
    "proxylist=[] #stores IPs \n",
    "webpage=[] #\n",
    "proxy_list=[] #stores IP pages \n",
    "count=0\n",
    "#get proxy pages\n",
    "for proxy_page in ListlinkerHref:\n",
    "    if \"FRESH\" in proxy_page.text :\n",
    "        print(count, proxy_page.get_attribute(\"href\"))\n",
    "        proxy_list.append(proxy_page.get_attribute(\"href\"))\n",
    "    count+=1\n",
    "#now we get the proxies themselves\n",
    "for proxy_page in proxy_list:\n",
    "    print(\"scraping page:\", proxy_page)\n",
    "    browser.get(proxy_page)\n",
    "    browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "    #get subpage urls \n",
    "    browser.implicitly_wait(10)\n",
    "    time.sleep(5)\n",
    "    s_scrape = browser.find_element_by_css_selector(\"textarea\") \n",
    "    for IP in (s_scrape.text).splitlines(): #add to list \n",
    "        proxylist.append(IP)\n",
    "        webpage.append(proxy_page)\n",
    "\n",
    "print(\"done scraping\")\n",
    "#now write to df \n",
    "df_proxy_list = pd.DataFrame(\n",
    "    np.column_stack([proxylist, webpage]), \n",
    "    columns=['proxy','webpage'])\n",
    "print(df_proxy_list.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "import requests \n",
    "def getProxy(ps_user, ps_pass, ps_host, ps_port, ps_db, update, **kwargs): \n",
    "    status=False\n",
    "    proxy=''\n",
    "    try: \n",
    "        with psycopg2.connect(user=ps_user,password=ps_pass,host=ps_host,port=ps_port,database=ps_db) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(\"select proxy from sc_land.sc_proxy_raw where status ='ready' order by table_id limit 1\")\n",
    "                result = cur.fetchone()\n",
    "                if update==True:\n",
    "                    cur.execute(\"update sc_land.sc_proxy_raw set status = 'used' where proxy = %(proxy)s\",\n",
    "                        {\n",
    "                            'proxy': result[0]\n",
    "                        }\n",
    "                    )\n",
    "                    conn.commit()\n",
    "        print(\"proxy used is:\", result[0])\n",
    "        proxy=result[0]\n",
    "        status=True\n",
    "    except Exception as e: \n",
    "        print(\"error on get next proxy:\", e)\n",
    "    return proxy, status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "proxy, status=getProxy(\"postgres\", \"root\", \"172.22.114.65\", \"5432\", \"scrape_db\", True)\n",
    "proxies= { 'http': 'http://' + proxy, 'https': 'https://' + proxy } \n",
    "timeout=3\n",
    "requests.get(baseurl, headers=headers, proxies=proxies, timeout=timeout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "session = requests.Session()\n",
    "retry = Retry(connect=3, backoff_factor=0.5)\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount('http://', adapter)\n",
    "session.mount('https://', adapter)\n",
    "\n",
    "# response = session.get(baseurl,headers=headers,proxies=proxies, timeout=Scrapewait)\n",
    "\n",
    "scrape_stat=False\n",
    "lc=0\n",
    "\n",
    "while scrape_stat==False:\n",
    "    lc+=1\n",
    "    proxy, status=getProxy(\"postgres\", \"root\", \"172.22.114.65\", \"5432\", \"scrape_db\", True)\n",
    "    proxies= { 'http': 'http://' + proxy, 'https': 'https://' + proxy } \n",
    "    H_ScrapeDT=(datetime.datetime.now())\n",
    "\n",
    "    print( (datetime.datetime.now()).strftime('%Y-%m-%d %H:%M:%S') )\n",
    "    try:\n",
    "        print(\"trying to scrape with: \", proxy)\n",
    "        response = session.get(baseurl,headers=headers,proxies=proxies, timeout=Scrapewait)\n",
    "        xmlFile=PageSaveFolder + XMLsaveFile \n",
    "        saveXML=open(xmlFile +'.xml', \"w\")\n",
    "        saveXML.close()\n",
    "        tree = etree.fromstring(response.content) \n",
    "        scrape_stat=True\n",
    "    except Exception as e: \n",
    "        print(\"try:\", str(lc),\"- error:\", str(e))\n",
    "        \n",
    "print(\"fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(xmlFile +'.xml', \"wb\") as saveXML:\n",
    "    saveXML.write(etree.tostring(tree,pretty_print=True))\n",
    "print(\"temp file saved to: \" + xmlFile +'.xml')\n",
    "H_FileSize=round(os.path.getsize(xmlFile +'.xml') / 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import pandas as pd\n",
    "import time \n",
    "import lxml \n",
    "import requests\n",
    "import base64\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from tkinter import Tk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "session = requests.Session()\n",
    "retry = Retry(connect=5, backoff_factor=0.5)\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount('http://', adapter)\n",
    "session.mount('https://', adapter)\n",
    "url='https://www.proxynova.com/proxy-server-list/anonymous-proxies/'\n",
    "response = session.get(url, headers=headers)#proxies={'http:' : 'http://139.224.47.77:8080', 'https:' : 'http://139.224.47.77:8080'})\n",
    "#proxy nova\n",
    "df_pn = pd.read_html(response.text) #'https://www.proxynova.com/proxy-server-list/anonymous-proxies/')\n",
    "df_pn = df_pn[0]\n",
    "df_pn = df_pn[~df_pn['Proxy IP'].isin(['(adsbygoogle = window.adsbygoogle || []).push({});'])]\n",
    "df_pn['IP'] = df_pn.apply(lambda x: str(x['Proxy IP']).replace('document.write(','').replace(');','').replace(\"'\",'') + \":\" + str(x['Proxy Port']), axis=1 )\n",
    "df_pn['IP'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver import ActionChains\n",
    "import time \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "url='https://proxyscrape.com/free-proxy-list'#'https://api.proxyscrape.com/v2/?request=share&protocol=socks4&timeout=400&country=all&simplified=true'\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_prefs = {}\n",
    "chrome_options.experimental_options[\"prefs\"] = chrome_prefs\n",
    "chrome_prefs[\"profile.default_content_settings\"] = {\"images\": 2}\n",
    "\n",
    "browser = webdriver.Chrome(options=chrome_options)\n",
    "browser.get(url)\n",
    "#get subpage urls \n",
    "browser.implicitly_wait(10)\n",
    "time.sleep(5)\n",
    "\n",
    "#close overlay if exists \n",
    "if len(browser.find_elements_by_xpath(\"//iframe[@class='HB-Takeover french-rose']\")) >0: #overlay active\n",
    "    print(\"clearing active overlay\")\n",
    "    time.sleep(15)\n",
    "    browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "    browser.switch_to_frame(browser.find_element_by_xpath(\"//iframe[@class='HB-Takeover french-rose']\"))\n",
    "    time.sleep(15)\n",
    "    click_stat=False\n",
    "    while click_stat ==False:\n",
    "        try:\n",
    "            browser.execute_script(\"arguments[0].click();\", browser.find_element_by_class_name(\"icon-close\"))\n",
    "            click_stat=True\n",
    "        except Exception as e: \n",
    "            print(\"error sleep 10, trying again:\", str(e))\n",
    "            time.sleep(10)\n",
    "    print(\"overlay cleared\")\n",
    "#get to proxy page\n",
    "browser.switch_to.default_content() #swap from iframe window\n",
    "# browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "browser.maximize_window()\n",
    "# https://stackoverflow.com/questions/40485157/how-to-move-range-input-using-selenium-in-python\n",
    "en =  browser.find_element_by_id(\"socks4timeoutslide\")\n",
    "move = ActionChains(browser)\n",
    "move.click_and_hold(en).move_by_offset(-95, 0).release().perform()\n",
    "button = browser.find_element_by_id(\"sharesocks4\") #browser.find_elements_by_class_name('downloadbtn')[1]\n",
    "button.click()\n",
    "#load new page\n",
    "browser.implicitly_wait(10)\n",
    "time.sleep(5)\n",
    "s_scrape = browser.find_element_by_css_selector(\"textarea\") \n",
    "proxylist=[] #stores IPs \n",
    "webpage=[] #\n",
    "proxy_list=[] #stores IP pages \n",
    "# count=0\n",
    "for IP in (s_scrape.text).splitlines(): #add to list \n",
    "        proxylist.append(IP)\n",
    "        webpage.append('proxy-list.download')\n",
    "print(\"done scraping\")\n",
    "#now write to df \n",
    "df_proxy_list = pd.DataFrame(\n",
    "    np.column_stack([proxylist, webpage]), \n",
    "    columns=['proxy','webpage'])\n",
    "print(df_proxy_list.head())\n",
    "browser.quit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver import ActionChains\n",
    "import time \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "url='https://www.proxy-list.download/SOCKS4'\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_prefs = {}\n",
    "chrome_options.experimental_options[\"prefs\"] = chrome_prefs\n",
    "chrome_prefs[\"profile.default_content_settings\"] = {\"images\": 2}\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")\n",
    "browser = webdriver.Chrome(options=chrome_options)\n",
    "browser.get(url)\n",
    "#get subpage urls \n",
    "browser.implicitly_wait(10)\n",
    "time.sleep(5)\n",
    "s_scrape = browser.find_element_by_id(\"downloadbtn\") #browser.find_element_by_css_selector(\"textarea\") \n",
    "s_scrape.click() #download as txt file \n",
    "df_proxy_list = pd.read_csv('Proxy List.txt',sep=\"\\t\", names=['proxy']) #import to df \n",
    "df_proxy_list['webage']='proxy-list.download'\n",
    "\n",
    "browser.quit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "url='https://www.proxynova.com/proxy-server-list/anonymous-proxies/'\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")\n",
    "browser = webdriver.Chrome(executable_path=r\"C:\\Users\\chapo\\Downloads\\chromedriver.exe\", chrome_options=chrome_options)\n",
    "browser.get(url)\n",
    "\n",
    "table = browser.find_element_by_id(\"tbl_proxy_list\")\n",
    "# df_pn = pd.read_html(browser.text) #'https://www.proxynova.com/proxy-server-list/anonymous-proxies/')\n",
    "proxylist=[]\n",
    "webpage=[]\n",
    "for a in table.text.splitlines():\n",
    "    if '.' in a: \n",
    "        a_split= a.split(\" \")\n",
    "        proxylist.append(a_split[0] + \":\" + a_split[1])\n",
    "        webpage.append('proxynova.com')\n",
    "print(\"done scraping\")\n",
    "\n",
    "#now write to df \n",
    "df_proxy_list = pd.DataFrame(\n",
    "    np.column_stack([proxylist, webpage]), \n",
    "    columns=['proxy','webpage'])\n",
    "print(df_proxy_list.head())\n",
    "\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver import ActionChains\n",
    "import time \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "url='https://www.proxynova.com/proxy-server-list/anonymous-proxies/'\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_prefs = {}\n",
    "chrome_options.experimental_options[\"prefs\"] = chrome_prefs\n",
    "chrome_prefs[\"profile.default_content_settings\"] = {\"images\": 2}\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")\n",
    "browser = webdriver.Chrome(options=chrome_options)\n",
    "browser.get(url)\n",
    "\n",
    "table = browser.find_element_by_id(\"tbl_proxy_list\")\n",
    "# df_pn = pd.read_html(browser.text) #'https://www.proxynova.com/proxy-server-list/anonymous-proxies/')\n",
    "proxylist=[]\n",
    "webpage=[]\n",
    "for a in table.text.splitlines():\n",
    "    if '.' in a: \n",
    "        a_split= a.split(\" \")\n",
    "        proxylist.append(a_split[0] + \":\" + a_split[1])\n",
    "        webpage.append('proxynova.com')\n",
    "\n",
    "print(\"done scraping\")\n",
    "#now write to df \n",
    "df_proxy_list = pd.DataFrame(\n",
    "    np.column_stack([proxylist, webpage]), \n",
    "    columns=['proxy','webpage'])\n",
    "\n",
    "print(df_proxy_list.head())\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxylist=[] #stores IPs \n",
    "webpage=[] #\n",
    "proxy_list=[] #stores IP pages \n",
    "# count=0\n",
    "for IP in (s_scrape.text).splitlines(): #add to list \n",
    "        proxylist.append(IP)\n",
    "        webpage.append('proxyscrape.com')\n",
    "print(\"done scraping\")\n",
    "#now write to df \n",
    "df_proxy_list = pd.DataFrame(\n",
    "    np.column_stack([proxylist, webpage]), \n",
    "    columns=['proxy','webpage'])\n",
    "print(df_proxy_list.head())\n",
    "# browser.quit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#close overlay if exists \n",
    "if len(browser.find_elements_by_xpath(\"//iframe[@class='HB-Takeover french-rose']\")) >0: #overlay active\n",
    "    print(\"clearing active overlay\")\n",
    "    time.sleep(15)\n",
    "    browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "    browser.switch_to_frame(browser.find_element_by_xpath(\"//iframe[@class='HB-Takeover french-rose']\"))\n",
    "    time.sleep(15)\n",
    "    click_stat=False\n",
    "    while click_stat ==False:\n",
    "        try:\n",
    "            browser.execute_script(\"arguments[0].click();\", browser.find_element_by_class_name(\"icon-close\"))\n",
    "            click_stat=True\n",
    "        except Exception as e: \n",
    "            print(\"error sleep 10, trying again:\", str(e))\n",
    "            time.sleep(10)\n",
    "    print(\"overlay cleared\")\n",
    "#get to proxy page\n",
    "browser.switch_to.default_content() #swap from iframe window\n",
    "# browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "browser.maximize_window()\n",
    "# https://stackoverflow.com/questions/40485157/how-to-move-range-input-using-selenium-in-python\n",
    "en =  browser.find_element_by_id(\"socks4timeoutslide\")\n",
    "move = ActionChains(browser)\n",
    "move.click_and_hold(en).move_by_offset(-95, 0).release().perform()\n",
    "button = browser.find_element_by_id(\"sharesocks4\") #browser.find_elements_by_class_name('downloadbtn')[1]\n",
    "button.click()\n",
    "#load new page\n",
    "browser.implicitly_wait(10)\n",
    "time.sleep(5)\n",
    "s_scrape = browser.find_element_by_css_selector(\"textarea\") \n",
    "proxylist=[] #stores IPs \n",
    "webpage=[] #\n",
    "proxy_list=[] #stores IP pages \n",
    "# count=0\n",
    "for IP in (s_scrape.text).splitlines(): #add to list \n",
    "        proxylist.append(IP)\n",
    "        webpage.append(proxyscrape.com)\n",
    "print(\"done scraping\")\n",
    "#now write to df \n",
    "df_proxy_list = pd.DataFrame(\n",
    "    np.column_stack([proxylist, webpage]), \n",
    "    columns=['proxy','webpage'])\n",
    "print(df_proxy_list.head())\n",
    "browser.quit() \n",
    "status=True\n",
    "    # except Exception as e:\n",
    "    #     print(\"error:\", str(e))\n",
    "    #     status=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver import ActionChains\n",
    "import time \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "url='https://proxyscrape.com/free-proxy-list'#'https://api.proxyscrape.com/v2/?request=share&protocol=socks4&timeout=400&country=all&simplified=true'\n",
    "status=False\n",
    "# while status==False:\n",
    "    # try: \n",
    "browser = webdriver.Chrome(executable_path=r\"C:\\Users\\chapo\\Downloads\\chromedriver.exe\")\n",
    "browser.get(url)\n",
    "#get subpage urls \n",
    "browser.implicitly_wait(10)\n",
    "time.sleep(5)\n",
    "\n",
    "#close overlay if exists \n",
    "if len(browser.find_elements_by_xpath(\"//iframe[@class='HB-Takeover french-rose']\")) >0: #overlay active\n",
    "    print(\"clearing active overlay\")\n",
    "    time.sleep(15)\n",
    "    browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "    browser.switch_to_frame(browser.find_element_by_xpath(\"//iframe[@class='HB-Takeover french-rose']\"))\n",
    "    time.sleep(15)\n",
    "    click_stat=False\n",
    "    while click_stat ==False:\n",
    "        try:\n",
    "            browser.execute_script(\"arguments[0].click();\", browser.find_element_by_class_name(\"icon-close\"))\n",
    "            click_stat=True\n",
    "        except Exception as e: \n",
    "            print(\"error sleep 10, trying again:\", str(e))\n",
    "            time.sleep(10)\n",
    "    print(\"overlay cleared\")\n",
    "#get to proxy page\n",
    "browser.switch_to.default_content() #swap from iframe window\n",
    "# browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "browser.maximize_window()\n",
    "# https://stackoverflow.com/questions/40485157/how-to-move-range-input-using-selenium-in-python\n",
    "en =  browser.find_element_by_id(\"socks4timeoutslide\")\n",
    "move = ActionChains(browser)\n",
    "move.click_and_hold(en).move_by_offset(-95, 0).release().perform()\n",
    "button = browser.find_element_by_id(\"sharesocks4\") #browser.find_elements_by_class_name('downloadbtn')[1]\n",
    "button.click()\n",
    "#load new page\n",
    "browser.implicitly_wait(10)\n",
    "time.sleep(5)\n",
    "s_scrape = browser.find_element_by_css_selector(\"textarea\") \n",
    "proxylist=[] #stores IPs \n",
    "webpage=[] #\n",
    "proxy_list=[] #stores IP pages \n",
    "# count=0\n",
    "for IP in (s_scrape.text).splitlines(): #add to list \n",
    "        proxylist.append(IP)\n",
    "        webpage.append(proxyscrape.com)\n",
    "print(\"done scraping\")\n",
    "#now write to df \n",
    "df_proxy_list = pd.DataFrame(\n",
    "    np.column_stack([proxylist, webpage]), \n",
    "    columns=['proxy','webpage'])\n",
    "print(df_proxy_list.head())\n",
    "browser.quit() \n",
    "status=True\n",
    "    # except Exception as e:\n",
    "    #     print(\"error:\", str(e))\n",
    "    #     status=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url='http://free-proxy.cz/en/proxylist/country/all/http/ping/level2'\n",
    "# df_res = pd.DataFrame()\n",
    "# _page = 1\n",
    "# headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "# session = requests.Session()\n",
    "# retry = Retry(connect=5, backoff_factor=0.5)\n",
    "# adapter = HTTPAdapter(max_retries=retry)\n",
    "# session.mount('http://', adapter)\n",
    "# session.mount('https://', adapter)\n",
    "# while _page <= 5: \n",
    "#     proxy_table = pd.DataFrame()\n",
    "#     time.sleep(4)\n",
    "#     print(url + '/' + str(_page))\n",
    "#     # session.get(baseurl)\n",
    "#     if _page ==1: response = session.get(url, headers=headers)\n",
    "#     else: response = session.get(url + '/' + str(_page), headers=headers)\n",
    "#     # parser = html.fromstring(response.text)\n",
    "#     IP=[]\n",
    "#     result=False\n",
    "#     root = lxml.etree.HTML(response.text)\n",
    "#     toner_id='proxy_list'\n",
    "#     results = root.xpath(\"//table[@id =@id = '%s']\" % toner_id)\n",
    "#     #parse table needed\n",
    "#     pd_tables = [pd.read_html(lxml.etree.tostring(table,method='html')) for table in results]\n",
    "#     proxy_table = pd_tables[0][0]\n",
    "    \n",
    "#     proxy_table = proxy_table[~proxy_table['IP address'].isin(['(adsbygoogle = window.adsbygoogle || []).push({});'])]\n",
    "#     proxy_table['IP address']=proxy_table['IP address'].apply(lambda x: str( str( base64.b64decode( str(x).replace('document.write(Base64.decode(\"','').replace('\"))','') ) ) ).replace('b','').replace(\"'\",'') ) \n",
    "#     proxy_table['IP_port']=proxy_table.apply(lambda x: x['IP address'] + ':' + x['Port'], axis=1)\n",
    "#     proxy_table['page_id']=_page\n",
    "#     print(\"page table len:\", str(len(proxy_table)))\n",
    "#     df_res = df_res.append(proxy_table)\n",
    "#     # print(\"done with:\", _page)\n",
    "#     _page += 1\n",
    "# session.close() \n",
    "# df_res.to_csv(r'C:\\Users\\chapo\\Downloads\\aa\\test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url='http://www.freeproxylists.net/?c=&pt=&pr=&a%5B%5D=1&a%5B%5D=2&u=50'\n",
    "# # df_fp = pd.read_html('http://free-proxy.cz/en/proxylist/country/all/http/ping/level2')\n",
    "# # https://www.proxynova.com/proxy-server-list/anonymous-proxies/\n",
    "# df_res = pd.DataFrame()\n",
    "# _page = 1\n",
    "# headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "# session = requests.Session()\n",
    "# retry = Retry(connect=5, backoff_factor=0.5)\n",
    "# adapter = HTTPAdapter(max_retries=retry)\n",
    "# session.mount('http://', adapter)\n",
    "# session.mount('https://', adapter)\n",
    "\n",
    "# # proxy_table = pd.DataFrame()\n",
    "# # time.sleep(4)\n",
    "# response = session.get(url, headers=headers, proxies={'http:' : 'http://139.224.47.77:8080', 'https:' : 'http://139.224.47.77:8080'})\n",
    "# # # parser = html.fromstring(response.text)\n",
    "# # IP=[]\n",
    "# # result=False\n",
    "# # root = lxml.etree.HTML(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url='http://free-proxy.cz/en/proxylist/country/all/http/ping/level2'\n",
    "# df_res = pd.DataFrame()\n",
    "# _page = 1\n",
    "# # headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "# session = requests.Session()\n",
    "# retry = Retry(connect=5, backoff_factor=0.5)\n",
    "# adapter = HTTPAdapter(max_retries=retry)\n",
    "# # session.mount('http://', adapter)\n",
    "# session.mount('https://', adapter)\n",
    "\n",
    "# proxy_table = pd.DataFrame()\n",
    "# time.sleep(4)\n",
    "# print(url + '/' + str(_page))\n",
    "# # session.get(baseurl)\n",
    "# if _page ==1: response = session.get(url, headers=headers)\n",
    "# else: response = session.get(url + '/' + str(_page), headers=headers)\n",
    "# # parser = html.fromstring(response.text)\n",
    "# IP=[]\n",
    "# result=False\n",
    "# root = lxml.etree.HTML(response.text)\n",
    "\n",
    "\n",
    "# toner_id='proxy_list'\n",
    "# results = root.xpath(\"//table[@id =@id = '%s']\" % toner_id)\n",
    "# #parse table needed\n",
    "# # pd_tables = [pd.read_html(lxml.etree.tostring(table,method='html')) for table in results]\n",
    "# # proxy_table = pd_tables[0][0]"
   ]
  }
 ]
}