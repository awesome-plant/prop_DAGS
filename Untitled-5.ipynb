{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import pandas as pd\n",
    "import time \n",
    "import lxml \n",
    "import requests\n",
    "import base64\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "session = requests.Session()\n",
    "retry = Retry(connect=5, backoff_factor=0.5)\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount('http://', adapter)\n",
    "session.mount('https://', adapter)\n",
    "url='https://www.proxynova.com/proxy-server-list/anonymous-proxies/'\n",
    "response = session.get(url, headers=headers)#proxies={'http:' : 'http://139.224.47.77:8080', 'https:' : 'http://139.224.47.77:8080'})\n",
    "#proxy nova\n",
    "df_pn = pd.read_html(response.text) #'https://www.proxynova.com/proxy-server-list/anonymous-proxies/')\n",
    "df_pn = df_pn[0]\n",
    "df_pn = df_pn[~df_pn['Proxy IP'].isin(['(adsbygoogle = window.adsbygoogle || []).push({});'])]\n",
    "df_pn['IP'] = df_pn.apply(lambda x: str(x['Proxy IP']).replace('document.write(','').replace(');','').replace(\"'\",'') + \":\" + str(x['Proxy Port']), axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docker script\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_prefs = {}\n",
    "chrome_options.experimental_options[\"prefs\"] = chrome_prefs\n",
    "chrome_prefs[\"profile.default_content_settings\"] = {\"images\": 2}\n",
    "\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "url='https://openproxy.space/list'\n",
    "browser = webdriver.Chrome(options=chrome_options)\n",
    "browser.get(url)\n",
    "browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "#get subpage urls \n",
    "browser.implicitly_wait(10)\n",
    "time.sleep(5)\n",
    "ListlinkerHref = browser.find_elements_by_xpath(\"//*[@href]\")\n",
    "proxylist=[] #stores IPs \n",
    "webpage=[] #\n",
    "proxy_list=[] #stores IP pages \n",
    "count=0\n",
    "#get proxy pages\n",
    "for proxy_page in ListlinkerHref:\n",
    "    if \"FRESH\" in proxy_page.text :\n",
    "        print(count, proxy_page.get_attribute(\"href\"))\n",
    "        proxy_list.append(proxy_page.get_attribute(\"href\"))\n",
    "    count+=1\n",
    "#now we get the proxies themselves\n",
    "for proxy_page in proxy_list:\n",
    "    print(\"scraping page:\", proxy_page)\n",
    "    browser.get(proxy_page)\n",
    "    browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "    #get subpage urls \n",
    "    browser.implicitly_wait(10)\n",
    "    time.sleep(5)\n",
    "    s_scrape = browser.find_element_by_css_selector(\"textarea\") \n",
    "    for IP in (s_scrape.text).splitlines(): #add to list \n",
    "        proxylist.append(IP)\n",
    "        webpage.append(proxy_page)\n",
    "\n",
    "print(\"done scraping\")\n",
    "#now write to df \n",
    "df_proxy_list = pd.DataFrame(\n",
    "    np.column_stack([proxylist, webpage]), \n",
    "    columns=['proxy','webpage'])\n",
    "print(df_proxy_list.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "url='https://proxyscrape.com/free-proxy-list-a'#'https://api.proxyscrape.com/v2/?request=share&protocol=socks4&timeout=400&country=all&simplified=true'\n",
    "\n",
    "browser = webdriver.Chrome(executable_path=r\"C:\\Users\\chapo\\Downloads\\chromedriver.exe\")\n",
    "browser.get(url)\n",
    "#get subpage urls \n",
    "browser.implicitly_wait(10)\n",
    "time.sleep(5)\n",
    "\n",
    "#close overlay if exists \n",
    "if len(browser.find_elements_by_xpath(\"//iframe[@class='HB-Takeover french-rose']\")) >0: #overlay active\n",
    "    \n",
    "    \n",
    "\n",
    "#get to proxy page\n",
    "# https://stackoverflow.com/questions/40485157/how-to-move-range-input-using-selenium-in-python\n",
    "from selenium.webdriver import ActionChains\n",
    "en =  browser.find_element_by_id(\"socks4timeoutslide\")\n",
    "move = ActionChains(browser)\n",
    "move.click_and_hold(en).move_by_offset(-95, 0).release().perform()\n",
    "button = browser.find_element_by_id(\"sharesocks4\") #browser.find_elements_by_class_name('downloadbtn')[1]\n",
    "button.click()\n",
    "#load new page\n",
    "browser.implicitly_wait(10)\n",
    "time.sleep(5)\n",
    "s_scrape = browser.find_element_by_css_selector(\"textarea\") \n",
    "proxylist=[] #stores IPs \n",
    "webpage=[] #\n",
    "proxy_list=[] #stores IP pages \n",
    "# count=0\n",
    "for IP in (s_scrape.text).splitlines(): #add to list \n",
    "        proxylist.append(IP)\n",
    "print(\"done scraping\")\n",
    "#now write to df \n",
    "df_proxy_list = pd.DataFrame(\n",
    "    np.column_stack([proxylist, webpage]), \n",
    "    columns=['proxy','webpage'])\n",
    "print(df_proxy_list.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_button=browser.find_elements_by_class_name(\"icon-close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "len(close_button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url='http://free-proxy.cz/en/proxylist/country/all/http/ping/level2'\n",
    "# df_res = pd.DataFrame()\n",
    "# _page = 1\n",
    "# headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "# session = requests.Session()\n",
    "# retry = Retry(connect=5, backoff_factor=0.5)\n",
    "# adapter = HTTPAdapter(max_retries=retry)\n",
    "# session.mount('http://', adapter)\n",
    "# session.mount('https://', adapter)\n",
    "# while _page <= 5: \n",
    "#     proxy_table = pd.DataFrame()\n",
    "#     time.sleep(4)\n",
    "#     print(url + '/' + str(_page))\n",
    "#     # session.get(baseurl)\n",
    "#     if _page ==1: response = session.get(url, headers=headers)\n",
    "#     else: response = session.get(url + '/' + str(_page), headers=headers)\n",
    "#     # parser = html.fromstring(response.text)\n",
    "#     IP=[]\n",
    "#     result=False\n",
    "#     root = lxml.etree.HTML(response.text)\n",
    "#     toner_id='proxy_list'\n",
    "#     results = root.xpath(\"//table[@id =@id = '%s']\" % toner_id)\n",
    "#     #parse table needed\n",
    "#     pd_tables = [pd.read_html(lxml.etree.tostring(table,method='html')) for table in results]\n",
    "#     proxy_table = pd_tables[0][0]\n",
    "    \n",
    "#     proxy_table = proxy_table[~proxy_table['IP address'].isin(['(adsbygoogle = window.adsbygoogle || []).push({});'])]\n",
    "#     proxy_table['IP address']=proxy_table['IP address'].apply(lambda x: str( str( base64.b64decode( str(x).replace('document.write(Base64.decode(\"','').replace('\"))','') ) ) ).replace('b','').replace(\"'\",'') ) \n",
    "#     proxy_table['IP_port']=proxy_table.apply(lambda x: x['IP address'] + ':' + x['Port'], axis=1)\n",
    "#     proxy_table['page_id']=_page\n",
    "#     print(\"page table len:\", str(len(proxy_table)))\n",
    "#     df_res = df_res.append(proxy_table)\n",
    "#     # print(\"done with:\", _page)\n",
    "#     _page += 1\n",
    "# session.close() \n",
    "# df_res.to_csv(r'C:\\Users\\chapo\\Downloads\\aa\\test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spys.one/en/https-ssl-proxy/\n",
    "# DNU http://free-proxy.cz/en/proxylist/country/all/http/ping/level2/2\n",
    "# DNU http://www.freeproxylists.net/?c=&pt=&pr=&a%5B%5D=1&a%5B%5D=2&u=50\n",
    "# DONE https://openproxy.space/list\n",
    "# https://proxyscrape.com/free-proxy-list\n",
    "# https://www.proxy-list.download/\n",
    "# done - https://www.proxynova.com/proxy-server-list/anonymous-proxies/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url='http://www.freeproxylists.net/?c=&pt=&pr=&a%5B%5D=1&a%5B%5D=2&u=50'\n",
    "# # df_fp = pd.read_html('http://free-proxy.cz/en/proxylist/country/all/http/ping/level2')\n",
    "# # https://www.proxynova.com/proxy-server-list/anonymous-proxies/\n",
    "# df_res = pd.DataFrame()\n",
    "# _page = 1\n",
    "# headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "# session = requests.Session()\n",
    "# retry = Retry(connect=5, backoff_factor=0.5)\n",
    "# adapter = HTTPAdapter(max_retries=retry)\n",
    "# session.mount('http://', adapter)\n",
    "# session.mount('https://', adapter)\n",
    "\n",
    "# # proxy_table = pd.DataFrame()\n",
    "# # time.sleep(4)\n",
    "# response = session.get(url, headers=headers, proxies={'http:' : 'http://139.224.47.77:8080', 'https:' : 'http://139.224.47.77:8080'})\n",
    "# # # parser = html.fromstring(response.text)\n",
    "# # IP=[]\n",
    "# # result=False\n",
    "# # root = lxml.etree.HTML(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url='http://free-proxy.cz/en/proxylist/country/all/http/ping/level2'\n",
    "# df_res = pd.DataFrame()\n",
    "# _page = 1\n",
    "# # headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "# session = requests.Session()\n",
    "# retry = Retry(connect=5, backoff_factor=0.5)\n",
    "# adapter = HTTPAdapter(max_retries=retry)\n",
    "# # session.mount('http://', adapter)\n",
    "# session.mount('https://', adapter)\n",
    "\n",
    "# proxy_table = pd.DataFrame()\n",
    "# time.sleep(4)\n",
    "# print(url + '/' + str(_page))\n",
    "# # session.get(baseurl)\n",
    "# if _page ==1: response = session.get(url, headers=headers)\n",
    "# else: response = session.get(url + '/' + str(_page), headers=headers)\n",
    "# # parser = html.fromstring(response.text)\n",
    "# IP=[]\n",
    "# result=False\n",
    "# root = lxml.etree.HTML(response.text)\n",
    "\n",
    "\n",
    "# toner_id='proxy_list'\n",
    "# results = root.xpath(\"//table[@id =@id = '%s']\" % toner_id)\n",
    "# #parse table needed\n",
    "# # pd_tables = [pd.read_html(lxml.etree.tostring(table,method='html')) for table in results]\n",
    "# # proxy_table = pd_tables[0][0]"
   ]
  }
 ]
}