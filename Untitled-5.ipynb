{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNU https://spys.one/en/https-ssl-proxy/\n",
    "# DNU http://free-proxy.cz/en/proxylist/country/all/http/ping/level2/2\n",
    "# DNU http://www.freeproxylists.net/?c=&pt=&pr=&a%5B%5D=1&a%5B%5D=2&u=50\n",
    "# -DONE https://openproxy.space/list\n",
    "# -DONE https://proxyscrape.com/free-proxy-list\n",
    "# -DONE https://www.proxy-list.download/\n",
    "# DONE https://www.proxynova.com/proxy-server-list/anonymous-proxies/\n",
    "# https://www.proxyscan.io/\n",
    "# https://free-proxy-list.net/anonymous-proxy.html\n",
    "# https://www.hidemyass-freeproxy.com/ ? \n",
    "# https://nl.hideproxy.me/ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "from psycopg2 import Error\n",
    "import csv\n",
    "from io import StringIO\n",
    "def getProxies(ps_user, ps_pass, ps_host, ps_port, ps_db, sql_start, sql_size):\n",
    "    import pandas as pd\n",
    "    #queries db and returns list of all proxies within paramaters \n",
    "    with psycopg2.connect(user=ps_user,password=ps_pass,host=ps_host,port=ps_port,database=ps_db) as conn:\n",
    "        check_proxy_list=pd.read_sql_query(\"SELECT proxy FROM sc_land.sc_proxy_raw order by table_id limit \" + str(sql_size) + \" offset \" + str(sql_start), conn)\n",
    "        print(\"got row list, start:\", str(sql_start), 'length:', str(sql_size))\n",
    "    return check_proxy_list \n",
    "def testProxy(proxy, timeout, my_ip, **kwargs):\n",
    "    # def here returns proxy, confirmed with different whatismyip return \n",
    "    #return true when dif\n",
    "    status=False\n",
    "    error=None\n",
    "    headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "    proxies= { 'http': 'http://' + proxy, 'https': 'https://' + proxy } \n",
    "    url='https://ident.me/'\n",
    "    try:\n",
    "        r = requests.get(url, headers=headers, proxies=proxies, timeout=timeout)\n",
    "        _newIP = r.text\n",
    "        if my_ip !=_newIP: #IP masked\n",
    "            try: \n",
    "                site_url='https://www.realestate.com.au/'\n",
    "                r = requests.get(site_url, headers=headers, proxies=proxies, timeout=timeout)\n",
    "                status=True\n",
    "            except Exception as e:\n",
    "                error = url + '-' + str(e) \n",
    "        else: error = url + '-no IP mask'\n",
    "    except Exception as e: \n",
    "        error = url + '-' + str(e) \n",
    "    # print(\"proxy:\", proxy, '-result:', status, '-error:', error)\n",
    "    return np.array([status, error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "got row list, start: 0 length: 10\n"
     ]
    }
   ],
   "source": [
    "check_proxy_list = getProxies(\n",
    "        ps_user=\"postgres\"\n",
    "        , ps_pass=\"root\"\n",
    "        , ps_host=\"172.22.114.65\"\n",
    "        , ps_port=\"5432\"\n",
    "        , ps_db=\"scrape_db\"\n",
    "        , sql_start=0\n",
    "        , sql_size=10\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "myIP is: 124.170.26.96\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "myIP='124.170.26.96'\n",
    "print(\"myIP is:\", myIP)\n",
    "\n",
    "l_proxy=[]\n",
    "l_status=[]\n",
    "l_error=[]\n",
    "for index, row in check_proxy_list.iterrows(): #dont judge me \n",
    "    print(row['proxy'])\n",
    "    status= error=''\n",
    "    status, error = testProxy(proxy=row['proxy'],timeout=3, my_ip=myIP)\n",
    "    l_proxy.append(row['proxy'])\n",
    "    l_status.append(status)\n",
    "    l_error.append(error)\n",
    "\n",
    "df_proxy_list = pd.DataFrame(\n",
    "        np.column_stack([l_proxy, l_status,l_error]), \n",
    "        columns=['proxy','status','error'])\n",
    "\n",
    "with pd.option_context('display.max_rows', len(check_proxy_list), 'display.max_columns', None):  # more options can be specified also\n",
    "    print(\"proxy list cols:\", df_proxy_list.dtypes)\n",
    "    print(df_proxy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "14.97.2.107:80\n",
      "51.158.119.88:8811\n",
      "51.158.119.88:8761\n",
      "116.117.134.134:8081\n",
      "51.116.234.251:3128\n",
      "116.117.134.134:82\n",
      "116.117.134.134:81\n",
      "116.117.134.134:80\n",
      "203.162.21.216:8000\n",
      "136.233.215.137:80\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "proxy list cols: proxy     object\nstatus    object\nerror     object\ndtype: object\n                  proxy status  \\\n0        14.97.2.107:80  False   \n1    51.158.119.88:8811   True   \n2    51.158.119.88:8761   True   \n3  116.117.134.134:8081  False   \n4   51.116.234.251:3128  False   \n5    116.117.134.134:82  False   \n6    116.117.134.134:81  False   \n7    116.117.134.134:80  False   \n8   203.162.21.216:8000  False   \n9    136.233.215.137:80  False   \n\n                                               error  \n0  https://ident.me/-HTTPSConnectionPool(host='id...  \n1                                               None  \n2                                               None  \n3  https://ident.me/-HTTPSConnectionPool(host='id...  \n4  https://ident.me/-HTTPSConnectionPool(host='id...  \n5  https://ident.me/-HTTPSConnectionPool(host='id...  \n6  https://ident.me/-HTTPSConnectionPool(host='id...  \n7  https://ident.me/-HTTPSConnectionPool(host='id...  \n8  https://ident.me/-HTTPSConnectionPool(host='id...  \n9  https://ident.me/-HTTPSConnectionPool(host='id...  \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time \n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "url='https://ident.me/'\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")\n",
    "browser = webdriver.Chrome(executable_path=r\"C:\\Users\\chapo\\Downloads\\chromedriver.exe\", chrome_options=chrome_options)\n",
    "browser.get(url)\n",
    "\n",
    "IP = re.sub('<[^<]+?>', '', browser.page_source)\n",
    "print(IP)\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "IP = re.sub('<[^<]+?>', '', browser.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "timeout =3\n",
    "proxy = '58.147.170.132:10801'\n",
    "result=''\n",
    "\n",
    "\n",
    "_actualIP=q.text\n",
    "_newIP=_actualIP\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "proxies= { 'http': 'http://' + proxy, 'https': 'https://' + proxy } \n",
    "url='https://ident.me/'\n",
    "q=requests.get(url, headers=headers)\n",
    "\n",
    "try:\n",
    "    r = requests.get(url, headers=headers, proxies=proxies, timeout=timeout)\n",
    "    _newIP = r.text\n",
    "    if _actualIP !=_newIP: #IP masked\n",
    "        print(\"worked\")\n",
    "        try: \n",
    "            \n",
    "            site_url='https://www.realestate.com.au/'\n",
    "            r = requests.get(site_url, headers=headers, proxies=proxies, timeout=timeout)\n",
    "            # print(\"IP:\", proxy, \"-capable of scraping:\",site_url)\n",
    "            result=True\n",
    "        except Exception as e:\n",
    "            result='error: ' + str(e)\n",
    "    else: print(\"wrong IP:\", r.text)\n",
    "except Exception as e:\n",
    "    print(url, e) \n",
    "    # pass\n",
    "# if result==True: print(\"IP:\", proxy, \"-capable of scraping:\",site_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime \n",
    "import time \n",
    "import random\n",
    "baseurl='https://www.realestate.com.au/xml-sitemap/'\n",
    "        # , 'RootDir': '/opt/airflow/logs/XML_save_folder' \n",
    "PageSaveFolder=r'D:\\shared_folder\\airflow_logs\\XML_save_folder\\gz_files'#'/opt/airflow/logs/XML_save_folder/raw_sitemap/'\n",
    "Scrapewait=5\n",
    "# def ScrapeURL(baseurl, PageSaveFolder, Scrapewait, **kwargs):  \n",
    "XMLsaveFile=\"XML_scrape_\" + (datetime.datetime.now()).strftime('%Y-%m-%d')\n",
    "time.sleep(random.randint(1,10))\n",
    "# ua = UserAgent()\n",
    "#headers = {'User-Agent':str(ua.random)}\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "url='https://free-proxy-list.net/anonymous-proxy.html'\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")\n",
    "browser = webdriver.Chrome(executable_path=r\"C:\\Users\\chapo\\Downloads\\chromedriver.exe\", chrome_options=chrome_options)\n",
    "browser.get(url)\n",
    "\n",
    "browser.find_element_by_xpath(\"//select[@name='proxylisttable_length']/option[text()='80']\").click()\n",
    "\n",
    "table_MN = pd.read_html(browser.page_source)\n",
    "browser.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_MN[0][table_MN[0]['IP Address'].notnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docker script\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver import ActionChains\n",
    "import time \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_prefs = {}\n",
    "chrome_options.experimental_options[\"prefs\"] = chrome_prefs\n",
    "chrome_prefs[\"profile.default_content_settings\"] = {\"images\": 2}\n",
    "\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "url='https://openproxy.space/list'\n",
    "browser = webdriver.Chrome(options=chrome_options)\n",
    "browser.get(url)\n",
    "browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "#get subpage urls \n",
    "browser.implicitly_wait(10)\n",
    "time.sleep(5)\n",
    "ListlinkerHref = browser.find_elements_by_xpath(\"//*[@href]\")\n",
    "proxylist=[] #stores IPs \n",
    "webpage=[] #\n",
    "proxy_list=[] #stores IP pages \n",
    "count=0\n",
    "#get proxy pages\n",
    "for proxy_page in ListlinkerHref:\n",
    "    if \"FRESH\" in proxy_page.text :\n",
    "        print(count, proxy_page.get_attribute(\"href\"))\n",
    "        proxy_list.append(proxy_page.get_attribute(\"href\"))\n",
    "    count+=1\n",
    "#now we get the proxies themselves\n",
    "for proxy_page in proxy_list:\n",
    "    print(\"scraping page:\", proxy_page)\n",
    "    browser.get(proxy_page)\n",
    "    browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "    #get subpage urls \n",
    "    browser.implicitly_wait(10)\n",
    "    time.sleep(5)\n",
    "    s_scrape = browser.find_element_by_css_selector(\"textarea\") \n",
    "    for IP in (s_scrape.text).splitlines(): #add to list \n",
    "        proxylist.append(IP)\n",
    "        webpage.append(proxy_page)\n",
    "\n",
    "print(\"done scraping\")\n",
    "#now write to df \n",
    "df_proxy_list = pd.DataFrame(\n",
    "    np.column_stack([proxylist, webpage]), \n",
    "    columns=['proxy','webpage'])\n",
    "print(df_proxy_list.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "import requests \n",
    "def getProxy(ps_user, ps_pass, ps_host, ps_port, ps_db, update, **kwargs): \n",
    "    status=False\n",
    "    proxy=''\n",
    "    try: \n",
    "        with psycopg2.connect(user=ps_user,password=ps_pass,host=ps_host,port=ps_port,database=ps_db) as conn:\n",
    "            with conn.cursor() as cur:\n",
    "                cur.execute(\"select proxy from sc_land.sc_proxy_raw where status ='ready' order by table_id limit 1\")\n",
    "                result = cur.fetchone()\n",
    "                if update==True:\n",
    "                    cur.execute(\"update sc_land.sc_proxy_raw set status = 'used' where proxy = %(proxy)s\",\n",
    "                        {\n",
    "                            'proxy': result[0]\n",
    "                        }\n",
    "                    )\n",
    "                    conn.commit()\n",
    "        print(\"proxy used is:\", result[0])\n",
    "        proxy=result[0]\n",
    "        status=True\n",
    "    except Exception as e: \n",
    "        print(\"error on get next proxy:\", e)\n",
    "    return proxy, status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "proxy, status=getProxy(\"postgres\", \"root\", \"172.22.114.65\", \"5432\", \"scrape_db\", True)\n",
    "proxies= { 'http': 'http://' + proxy, 'https': 'https://' + proxy } \n",
    "timeout=3\n",
    "requests.get(baseurl, headers=headers, proxies=proxies, timeout=timeout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "session = requests.Session()\n",
    "retry = Retry(connect=3, backoff_factor=0.5)\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount('http://', adapter)\n",
    "session.mount('https://', adapter)\n",
    "\n",
    "# response = session.get(baseurl,headers=headers,proxies=proxies, timeout=Scrapewait)\n",
    "\n",
    "scrape_stat=False\n",
    "lc=0\n",
    "\n",
    "while scrape_stat==False:\n",
    "    lc+=1\n",
    "    proxy, status=getProxy(\"postgres\", \"root\", \"172.22.114.65\", \"5432\", \"scrape_db\", True)\n",
    "    proxies= { 'http': 'http://' + proxy, 'https': 'https://' + proxy } \n",
    "    H_ScrapeDT=(datetime.datetime.now())\n",
    "\n",
    "    print( (datetime.datetime.now()).strftime('%Y-%m-%d %H:%M:%S') )\n",
    "    try:\n",
    "        print(\"trying to scrape with: \", proxy)\n",
    "        response = session.get(baseurl,headers=headers,proxies=proxies, timeout=Scrapewait)\n",
    "        xmlFile=PageSaveFolder + XMLsaveFile \n",
    "        saveXML=open(xmlFile +'.xml', \"w\")\n",
    "        saveXML.close()\n",
    "        tree = etree.fromstring(response.content) \n",
    "        scrape_stat=True\n",
    "    except Exception as e: \n",
    "        print(\"try:\", str(lc),\"- error:\", str(e))\n",
    "        \n",
    "print(\"fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(xmlFile +'.xml', \"wb\") as saveXML:\n",
    "    saveXML.write(etree.tostring(tree,pretty_print=True))\n",
    "print(\"temp file saved to: \" + xmlFile +'.xml')\n",
    "H_FileSize=round(os.path.getsize(xmlFile +'.xml') / 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import pandas as pd\n",
    "import time \n",
    "import lxml \n",
    "import requests\n",
    "import base64\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from tkinter import Tk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "session = requests.Session()\n",
    "retry = Retry(connect=5, backoff_factor=0.5)\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount('http://', adapter)\n",
    "session.mount('https://', adapter)\n",
    "url='https://www.proxynova.com/proxy-server-list/anonymous-proxies/'\n",
    "response = session.get(url, headers=headers)#proxies={'http:' : 'http://139.224.47.77:8080', 'https:' : 'http://139.224.47.77:8080'})\n",
    "#proxy nova\n",
    "df_pn = pd.read_html(response.text) #'https://www.proxynova.com/proxy-server-list/anonymous-proxies/')\n",
    "df_pn = df_pn[0]\n",
    "df_pn = df_pn[~df_pn['Proxy IP'].isin(['(adsbygoogle = window.adsbygoogle || []).push({});'])]\n",
    "df_pn['IP'] = df_pn.apply(lambda x: str(x['Proxy IP']).replace('document.write(','').replace(');','').replace(\"'\",'') + \":\" + str(x['Proxy Port']), axis=1 )\n",
    "df_pn['IP'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver import ActionChains\n",
    "import time \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "url='https://proxyscrape.com/free-proxy-list'#'https://api.proxyscrape.com/v2/?request=share&protocol=socks4&timeout=400&country=all&simplified=true'\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_prefs = {}\n",
    "chrome_options.experimental_options[\"prefs\"] = chrome_prefs\n",
    "chrome_prefs[\"profile.default_content_settings\"] = {\"images\": 2}\n",
    "\n",
    "browser = webdriver.Chrome(options=chrome_options)\n",
    "browser.get(url)\n",
    "#get subpage urls \n",
    "browser.implicitly_wait(10)\n",
    "time.sleep(5)\n",
    "\n",
    "#close overlay if exists \n",
    "if len(browser.find_elements_by_xpath(\"//iframe[@class='HB-Takeover french-rose']\")) >0: #overlay active\n",
    "    print(\"clearing active overlay\")\n",
    "    time.sleep(15)\n",
    "    browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "    browser.switch_to_frame(browser.find_element_by_xpath(\"//iframe[@class='HB-Takeover french-rose']\"))\n",
    "    time.sleep(15)\n",
    "    click_stat=False\n",
    "    while click_stat ==False:\n",
    "        try:\n",
    "            browser.execute_script(\"arguments[0].click();\", browser.find_element_by_class_name(\"icon-close\"))\n",
    "            click_stat=True\n",
    "        except Exception as e: \n",
    "            print(\"error sleep 10, trying again:\", str(e))\n",
    "            time.sleep(10)\n",
    "    print(\"overlay cleared\")\n",
    "#get to proxy page\n",
    "browser.switch_to.default_content() #swap from iframe window\n",
    "# browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "browser.maximize_window()\n",
    "# https://stackoverflow.com/questions/40485157/how-to-move-range-input-using-selenium-in-python\n",
    "en =  browser.find_element_by_id(\"socks4timeoutslide\")\n",
    "move = ActionChains(browser)\n",
    "move.click_and_hold(en).move_by_offset(-95, 0).release().perform()\n",
    "button = browser.find_element_by_id(\"sharesocks4\") #browser.find_elements_by_class_name('downloadbtn')[1]\n",
    "button.click()\n",
    "#load new page\n",
    "browser.implicitly_wait(10)\n",
    "time.sleep(5)\n",
    "s_scrape = browser.find_element_by_css_selector(\"textarea\") \n",
    "proxylist=[] #stores IPs \n",
    "webpage=[] #\n",
    "proxy_list=[] #stores IP pages \n",
    "# count=0\n",
    "for IP in (s_scrape.text).splitlines(): #add to list \n",
    "        proxylist.append(IP)\n",
    "        webpage.append('proxy-list.download')\n",
    "print(\"done scraping\")\n",
    "#now write to df \n",
    "df_proxy_list = pd.DataFrame(\n",
    "    np.column_stack([proxylist, webpage]), \n",
    "    columns=['proxy','webpage'])\n",
    "print(df_proxy_list.head())\n",
    "browser.quit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver import ActionChains\n",
    "import time \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "url='https://www.proxy-list.download/SOCKS4'\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_prefs = {}\n",
    "chrome_options.experimental_options[\"prefs\"] = chrome_prefs\n",
    "chrome_prefs[\"profile.default_content_settings\"] = {\"images\": 2}\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")\n",
    "browser = webdriver.Chrome(options=chrome_options)\n",
    "browser.get(url)\n",
    "#get subpage urls \n",
    "browser.implicitly_wait(10)\n",
    "time.sleep(5)\n",
    "s_scrape = browser.find_element_by_id(\"downloadbtn\") #browser.find_element_by_css_selector(\"textarea\") \n",
    "s_scrape.click() #download as txt file \n",
    "df_proxy_list = pd.read_csv('Proxy List.txt',sep=\"\\t\", names=['proxy']) #import to df \n",
    "df_proxy_list['webage']='proxy-list.download'\n",
    "\n",
    "browser.quit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "url='https://www.proxynova.com/proxy-server-list/anonymous-proxies/'\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")\n",
    "browser = webdriver.Chrome(executable_path=r\"C:\\Users\\chapo\\Downloads\\chromedriver.exe\", chrome_options=chrome_options)\n",
    "browser.get(url)\n",
    "\n",
    "table = browser.find_element_by_id(\"tbl_proxy_list\")\n",
    "# df_pn = pd.read_html(browser.text) #'https://www.proxynova.com/proxy-server-list/anonymous-proxies/')\n",
    "proxylist=[]\n",
    "webpage=[]\n",
    "for a in table.text.splitlines():\n",
    "    if '.' in a: \n",
    "        a_split= a.split(\" \")\n",
    "        proxylist.append(a_split[0] + \":\" + a_split[1])\n",
    "        webpage.append('proxynova.com')\n",
    "print(\"done scraping\")\n",
    "\n",
    "#now write to df \n",
    "df_proxy_list = pd.DataFrame(\n",
    "    np.column_stack([proxylist, webpage]), \n",
    "    columns=['proxy','webpage'])\n",
    "print(df_proxy_list.head())\n",
    "\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver import ActionChains\n",
    "import time \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "url='https://www.proxynova.com/proxy-server-list/anonymous-proxies/'\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_prefs = {}\n",
    "chrome_options.experimental_options[\"prefs\"] = chrome_prefs\n",
    "chrome_prefs[\"profile.default_content_settings\"] = {\"images\": 2}\n",
    "chrome_options.add_argument(\"--disable-popup-blocking\")\n",
    "browser = webdriver.Chrome(options=chrome_options)\n",
    "browser.get(url)\n",
    "\n",
    "table = browser.find_element_by_id(\"tbl_proxy_list\")\n",
    "# df_pn = pd.read_html(browser.text) #'https://www.proxynova.com/proxy-server-list/anonymous-proxies/')\n",
    "proxylist=[]\n",
    "webpage=[]\n",
    "for a in table.text.splitlines():\n",
    "    if '.' in a: \n",
    "        a_split= a.split(\" \")\n",
    "        proxylist.append(a_split[0] + \":\" + a_split[1])\n",
    "        webpage.append('proxynova.com')\n",
    "\n",
    "print(\"done scraping\")\n",
    "#now write to df \n",
    "df_proxy_list = pd.DataFrame(\n",
    "    np.column_stack([proxylist, webpage]), \n",
    "    columns=['proxy','webpage'])\n",
    "\n",
    "print(df_proxy_list.head())\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxylist=[] #stores IPs \n",
    "webpage=[] #\n",
    "proxy_list=[] #stores IP pages \n",
    "# count=0\n",
    "for IP in (s_scrape.text).splitlines(): #add to list \n",
    "        proxylist.append(IP)\n",
    "        webpage.append('proxyscrape.com')\n",
    "print(\"done scraping\")\n",
    "#now write to df \n",
    "df_proxy_list = pd.DataFrame(\n",
    "    np.column_stack([proxylist, webpage]), \n",
    "    columns=['proxy','webpage'])\n",
    "print(df_proxy_list.head())\n",
    "# browser.quit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#close overlay if exists \n",
    "if len(browser.find_elements_by_xpath(\"//iframe[@class='HB-Takeover french-rose']\")) >0: #overlay active\n",
    "    print(\"clearing active overlay\")\n",
    "    time.sleep(15)\n",
    "    browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "    browser.switch_to_frame(browser.find_element_by_xpath(\"//iframe[@class='HB-Takeover french-rose']\"))\n",
    "    time.sleep(15)\n",
    "    click_stat=False\n",
    "    while click_stat ==False:\n",
    "        try:\n",
    "            browser.execute_script(\"arguments[0].click();\", browser.find_element_by_class_name(\"icon-close\"))\n",
    "            click_stat=True\n",
    "        except Exception as e: \n",
    "            print(\"error sleep 10, trying again:\", str(e))\n",
    "            time.sleep(10)\n",
    "    print(\"overlay cleared\")\n",
    "#get to proxy page\n",
    "browser.switch_to.default_content() #swap from iframe window\n",
    "# browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "browser.maximize_window()\n",
    "# https://stackoverflow.com/questions/40485157/how-to-move-range-input-using-selenium-in-python\n",
    "en =  browser.find_element_by_id(\"socks4timeoutslide\")\n",
    "move = ActionChains(browser)\n",
    "move.click_and_hold(en).move_by_offset(-95, 0).release().perform()\n",
    "button = browser.find_element_by_id(\"sharesocks4\") #browser.find_elements_by_class_name('downloadbtn')[1]\n",
    "button.click()\n",
    "#load new page\n",
    "browser.implicitly_wait(10)\n",
    "time.sleep(5)\n",
    "s_scrape = browser.find_element_by_css_selector(\"textarea\") \n",
    "proxylist=[] #stores IPs \n",
    "webpage=[] #\n",
    "proxy_list=[] #stores IP pages \n",
    "# count=0\n",
    "for IP in (s_scrape.text).splitlines(): #add to list \n",
    "        proxylist.append(IP)\n",
    "        webpage.append(proxyscrape.com)\n",
    "print(\"done scraping\")\n",
    "#now write to df \n",
    "df_proxy_list = pd.DataFrame(\n",
    "    np.column_stack([proxylist, webpage]), \n",
    "    columns=['proxy','webpage'])\n",
    "print(df_proxy_list.head())\n",
    "browser.quit() \n",
    "status=True\n",
    "    # except Exception as e:\n",
    "    #     print(\"error:\", str(e))\n",
    "    #     status=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver import ActionChains\n",
    "import time \n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "url='https://proxyscrape.com/free-proxy-list'#'https://api.proxyscrape.com/v2/?request=share&protocol=socks4&timeout=400&country=all&simplified=true'\n",
    "status=False\n",
    "# while status==False:\n",
    "    # try: \n",
    "browser = webdriver.Chrome(executable_path=r\"C:\\Users\\chapo\\Downloads\\chromedriver.exe\")\n",
    "browser.get(url)\n",
    "#get subpage urls \n",
    "browser.implicitly_wait(10)\n",
    "time.sleep(5)\n",
    "\n",
    "#close overlay if exists \n",
    "if len(browser.find_elements_by_xpath(\"//iframe[@class='HB-Takeover french-rose']\")) >0: #overlay active\n",
    "    print(\"clearing active overlay\")\n",
    "    time.sleep(15)\n",
    "    browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "    browser.switch_to_frame(browser.find_element_by_xpath(\"//iframe[@class='HB-Takeover french-rose']\"))\n",
    "    time.sleep(15)\n",
    "    click_stat=False\n",
    "    while click_stat ==False:\n",
    "        try:\n",
    "            browser.execute_script(\"arguments[0].click();\", browser.find_element_by_class_name(\"icon-close\"))\n",
    "            click_stat=True\n",
    "        except Exception as e: \n",
    "            print(\"error sleep 10, trying again:\", str(e))\n",
    "            time.sleep(10)\n",
    "    print(\"overlay cleared\")\n",
    "#get to proxy page\n",
    "browser.switch_to.default_content() #swap from iframe window\n",
    "# browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "browser.maximize_window()\n",
    "# https://stackoverflow.com/questions/40485157/how-to-move-range-input-using-selenium-in-python\n",
    "en =  browser.find_element_by_id(\"socks4timeoutslide\")\n",
    "move = ActionChains(browser)\n",
    "move.click_and_hold(en).move_by_offset(-95, 0).release().perform()\n",
    "button = browser.find_element_by_id(\"sharesocks4\") #browser.find_elements_by_class_name('downloadbtn')[1]\n",
    "button.click()\n",
    "#load new page\n",
    "browser.implicitly_wait(10)\n",
    "time.sleep(5)\n",
    "s_scrape = browser.find_element_by_css_selector(\"textarea\") \n",
    "proxylist=[] #stores IPs \n",
    "webpage=[] #\n",
    "proxy_list=[] #stores IP pages \n",
    "# count=0\n",
    "for IP in (s_scrape.text).splitlines(): #add to list \n",
    "        proxylist.append(IP)\n",
    "        webpage.append(proxyscrape.com)\n",
    "print(\"done scraping\")\n",
    "#now write to df \n",
    "df_proxy_list = pd.DataFrame(\n",
    "    np.column_stack([proxylist, webpage]), \n",
    "    columns=['proxy','webpage'])\n",
    "print(df_proxy_list.head())\n",
    "browser.quit() \n",
    "status=True\n",
    "    # except Exception as e:\n",
    "    #     print(\"error:\", str(e))\n",
    "    #     status=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url='http://free-proxy.cz/en/proxylist/country/all/http/ping/level2'\n",
    "# df_res = pd.DataFrame()\n",
    "# _page = 1\n",
    "# headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "# session = requests.Session()\n",
    "# retry = Retry(connect=5, backoff_factor=0.5)\n",
    "# adapter = HTTPAdapter(max_retries=retry)\n",
    "# session.mount('http://', adapter)\n",
    "# session.mount('https://', adapter)\n",
    "# while _page <= 5: \n",
    "#     proxy_table = pd.DataFrame()\n",
    "#     time.sleep(4)\n",
    "#     print(url + '/' + str(_page))\n",
    "#     # session.get(baseurl)\n",
    "#     if _page ==1: response = session.get(url, headers=headers)\n",
    "#     else: response = session.get(url + '/' + str(_page), headers=headers)\n",
    "#     # parser = html.fromstring(response.text)\n",
    "#     IP=[]\n",
    "#     result=False\n",
    "#     root = lxml.etree.HTML(response.text)\n",
    "#     toner_id='proxy_list'\n",
    "#     results = root.xpath(\"//table[@id =@id = '%s']\" % toner_id)\n",
    "#     #parse table needed\n",
    "#     pd_tables = [pd.read_html(lxml.etree.tostring(table,method='html')) for table in results]\n",
    "#     proxy_table = pd_tables[0][0]\n",
    "    \n",
    "#     proxy_table = proxy_table[~proxy_table['IP address'].isin(['(adsbygoogle = window.adsbygoogle || []).push({});'])]\n",
    "#     proxy_table['IP address']=proxy_table['IP address'].apply(lambda x: str( str( base64.b64decode( str(x).replace('document.write(Base64.decode(\"','').replace('\"))','') ) ) ).replace('b','').replace(\"'\",'') ) \n",
    "#     proxy_table['IP_port']=proxy_table.apply(lambda x: x['IP address'] + ':' + x['Port'], axis=1)\n",
    "#     proxy_table['page_id']=_page\n",
    "#     print(\"page table len:\", str(len(proxy_table)))\n",
    "#     df_res = df_res.append(proxy_table)\n",
    "#     # print(\"done with:\", _page)\n",
    "#     _page += 1\n",
    "# session.close() \n",
    "# df_res.to_csv(r'C:\\Users\\chapo\\Downloads\\aa\\test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url='http://www.freeproxylists.net/?c=&pt=&pr=&a%5B%5D=1&a%5B%5D=2&u=50'\n",
    "# # df_fp = pd.read_html('http://free-proxy.cz/en/proxylist/country/all/http/ping/level2')\n",
    "# # https://www.proxynova.com/proxy-server-list/anonymous-proxies/\n",
    "# df_res = pd.DataFrame()\n",
    "# _page = 1\n",
    "# headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "# session = requests.Session()\n",
    "# retry = Retry(connect=5, backoff_factor=0.5)\n",
    "# adapter = HTTPAdapter(max_retries=retry)\n",
    "# session.mount('http://', adapter)\n",
    "# session.mount('https://', adapter)\n",
    "\n",
    "# # proxy_table = pd.DataFrame()\n",
    "# # time.sleep(4)\n",
    "# response = session.get(url, headers=headers, proxies={'http:' : 'http://139.224.47.77:8080', 'https:' : 'http://139.224.47.77:8080'})\n",
    "# # # parser = html.fromstring(response.text)\n",
    "# # IP=[]\n",
    "# # result=False\n",
    "# # root = lxml.etree.HTML(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url='http://free-proxy.cz/en/proxylist/country/all/http/ping/level2'\n",
    "# df_res = pd.DataFrame()\n",
    "# _page = 1\n",
    "# # headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"}\n",
    "# session = requests.Session()\n",
    "# retry = Retry(connect=5, backoff_factor=0.5)\n",
    "# adapter = HTTPAdapter(max_retries=retry)\n",
    "# # session.mount('http://', adapter)\n",
    "# session.mount('https://', adapter)\n",
    "\n",
    "# proxy_table = pd.DataFrame()\n",
    "# time.sleep(4)\n",
    "# print(url + '/' + str(_page))\n",
    "# # session.get(baseurl)\n",
    "# if _page ==1: response = session.get(url, headers=headers)\n",
    "# else: response = session.get(url + '/' + str(_page), headers=headers)\n",
    "# # parser = html.fromstring(response.text)\n",
    "# IP=[]\n",
    "# result=False\n",
    "# root = lxml.etree.HTML(response.text)\n",
    "\n",
    "\n",
    "# toner_id='proxy_list'\n",
    "# results = root.xpath(\"//table[@id =@id = '%s']\" % toner_id)\n",
    "# #parse table needed\n",
    "# # pd_tables = [pd.read_html(lxml.etree.tostring(table,method='html')) for table in results]\n",
    "# # proxy_table = pd_tables[0][0]"
   ]
  }
 ]
}