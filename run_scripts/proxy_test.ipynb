{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fp.fp import FreeProxy\n",
    "import requests \n",
    "from fake_useragent import UserAgent\n",
    "import time \n",
    "import datetime \n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import shutil \n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "from psycopg2 import Error\n",
    "import csv\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psql_insert_copy(table, conn, keys, data_iter):\n",
    "    \"\"\"\n",
    "    Execute SQL statement inserting data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    table : pandas.io.sql.SQLTable\n",
    "    conn : sqlalchemy.engine.Engine or sqlalchemy.engine.Connection\n",
    "    keys : list of str\n",
    "        Column names\n",
    "    data_iter : Iterable that iterates the values to be inserted\n",
    "    \"\"\"\n",
    "    # gets a DBAPI connection that can provide a cursor\n",
    "    dbapi_conn = conn.connection\n",
    "    with dbapi_conn.cursor() as cur:\n",
    "        s_buf = StringIO()\n",
    "        writer = csv.writer(s_buf)\n",
    "        writer.writerows(data_iter)\n",
    "        s_buf.seek(0)\n",
    "\n",
    "        columns = ', '.join('\"{}\"'.format(k) for k in keys)\n",
    "        if table.schema:\n",
    "            table_name = '{}.{}'.format(table.schema, table.name)\n",
    "        else:\n",
    "            table_name = table.name\n",
    "\n",
    "        sql = 'COPY {} ({}) FROM STDIN WITH CSV'.format(\n",
    "            table_name, columns)\n",
    "        cur.copy_expert(sql=sql, file=s_buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProxy(): \n",
    "    # def here returns proxy, confirmed with different whatismyip return \n",
    "    url='https://ident.me/'\n",
    "    ua = UserAgent()\n",
    "    q=requests.get(url)\n",
    "    _actualIP=q.text\n",
    "    _newIP=''\n",
    "    _getIP_time=time.process_time()\n",
    "    _try=0\n",
    "    _checkout=False\n",
    "    while _checkout==False: #_newIP != _actualIP :\n",
    "        headers = {'User-Agent':str(ua.random)}\n",
    "        start= time.process_time()\n",
    "        proxy = FreeProxy(rand=True).get()\n",
    "        taken = time.process_time() - start\n",
    "        proxies= { 'http': proxy, 'https': proxy } \n",
    "        try:\n",
    "            r = requests.get(url, headers=headers, proxies=proxies)\n",
    "            _newIP = r.text\n",
    "            print(\"realIP is: \", _actualIP, \" - proxy IP is:\", _newIP, \" - attempt no.\", str(_try))\n",
    "        except Exception as e: \n",
    "            print('error on proxy get, try again:', e)\n",
    "        if _actualIP !=_newIP:\n",
    "            _checkout=True\n",
    "        _try+=1\n",
    "    print(\"fin, total time\", str( time.process_time() - _getIP_time ) )\n",
    "    print(\"realIP is: \", _actualIP, \" - proxy IP is:\", _newIP, \" - attempt no.\", str(_try))\n",
    "    return proxies, _newIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def SaveScrape(baseurl, PageSaveFolder, ScrapeFile, **kwargs):\n",
    "    print('gz file:', ScrapeFile)\n",
    "    #download from sitemap, use dynamic variable \n",
    "    sitemap_url = baseurl #'https://www.realestate.com.au/xml-sitemap/'#pdp-sitemap-buy-1.xml.gz' \n",
    "    _file=ScrapeFile #im lazy, sue me\n",
    "    gz_save_name =_file[:-7] + '_' + (datetime.datetime.now()).strftime('%Y-%m-%d') + '.gz'\n",
    "    gz_url = sitemap_url + _file\n",
    "    gz_save_path = PageSaveFolder\n",
    "    urllib.request.urlretrieve(gz_url, gz_save_path + gz_save_name)\n",
    "\n",
    "    #save gz to dir for archiving \n",
    "    print(\"file:\", gz_save_name)\n",
    "    print(\"written to dir:\", gz_save_path + gz_save_name)\n",
    "    #feast upon that rich gooey xml \n",
    "    _xml_save = _file[:-7] + '_' + (datetime.datetime.now()).strftime('%Y-%m-%d') + '.xml'  \n",
    "    with gzip.open(gz_save_path + gz_save_name, 'rb') as f_in:\n",
    "        with open(gz_save_path + _xml_save, 'wb') as f_out: \n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "    #xml part \n",
    "    root = etree.parse(gz_save_path + _xml_save)\n",
    "    XML_gz_Dataset=pd.DataFrame(columns =['parent_gz','scrape_dt','url', 'proptype', 'state', 'suburb', 'prop_id', 'lastmod', 'external_ip', 's_fileid'])\n",
    "    _PropType=_State=_PropID=_LastMod=_split=_Url=\"\"\n",
    "    _external_ip = urllib.request.urlopen('https://ident.me').read().decode('utf8')\n",
    "    _count=1\n",
    "    _time=time.time()\n",
    "    #iterate xml\n",
    "    for element in root.iter():\n",
    "        if _count % 10000 == 0: \n",
    "            print(\"interval:\", str(_count-1),\" -total runtime:\", time.time()-_time)\n",
    "        #writes results to df, same as the previous module \n",
    "        if 'url' in element.tag and _Url != '':\n",
    "            XML_gz_Dataset=XML_gz_Dataset.append({\n",
    "                        'parent_gz': str(ScrapeFile)\n",
    "                        ,'scrape_dt' : (datetime.datetime.now()).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                        , 'url' : str(_Url)\n",
    "                        , 'proptype' : str(_PropType)\n",
    "                        , 'state' : str(_State)\n",
    "                        , 'suburb' : str(_Suburb)\n",
    "                        , 'prop_id' : str(_PropID)\n",
    "                        , 'lastmod': str(_LastMod)\n",
    "                        , 'external_ip': str(_external_ip)\n",
    "                        } ,ignore_index=True) \n",
    "            _PropType=_State=_PropID=_LastMod=_split=_Url=\"\"\n",
    "        if 'lastmod' in element.tag: \n",
    "            _LastMod = element.text\n",
    "        #just about everything gleaned from loac (url) tag\n",
    "        elif 'loc' in element.tag: \n",
    "            if '-tas-' in element.text: \n",
    "                _State='tas'\n",
    "            elif '-vic-' in element.text: \n",
    "                _State='vic'\n",
    "            elif '-nsw-' in element.text: \n",
    "                _State='nsw'\n",
    "            elif '-act-' in element.text: \n",
    "                _State='act'\n",
    "            elif '-qld-' in element.text: \n",
    "                _State='qld'\n",
    "            elif '-nt-' in element.text: \n",
    "                _State='nt'\n",
    "            elif '-sa-' in element.text: \n",
    "                _State='sa'\n",
    "            elif '-wa-' in element.text: \n",
    "                _State='wa'\n",
    "    \n",
    "            _Url = element.text\n",
    "            if _State=='': #sometimes the urls they give are wrong\n",
    "                print(\"incorrect url:\", str(element.text))\n",
    "                _PropType=''\n",
    "                _Suburb=''\n",
    "                _PropID=''\n",
    "            else: \n",
    "                _split=str(element.text).split(_State) \n",
    "                #had to do it this way so unconventional suburb names are still caught\n",
    "                _PropType = _split[0].replace('https://www.realestate.com.au/property-','')[:-1]\n",
    "                _split=str(element.text).split('-')\n",
    "                _Suburb=_split[len(_split) -2 ]\n",
    "                _PropID=_split[len(_split) -1 ]\n",
    "            _count+=1\n",
    "    XML_gz_Dataset.to_csv(gz_save_path + '\\\\parsed_csv\\\\' + _xml_save[:-3] + '_results' +'.csv')\n",
    "    print(\"file saved to: \" + gz_save_path + '\\\\parsed_csv\\\\' + _xml_save[:-3] + '_results' +'.csv')\n",
    "    XML_gz_Dataset['lastmod']=pd.to_datetime(XML_gz_Dataset['lastmod'])\n",
    "    #now we add to db table \n",
    "    #parent file link\n",
    "    connection = psycopg2.connect(user=\"postgres\",password=\"root\",host=\"172.22.114.65\",port=\"5432\",database=\"scrape_db\")\n",
    "    cursor = connection.cursor()\n",
    "    # with connection.cursor() as cursor:\n",
    "    cursor.execute(\"\"\"\n",
    "        select max(s_fileid)\n",
    "        FROM sc_land.sc_source_file\n",
    "        WHERE s_filename = %(s_filename)s\n",
    "        and date(lastmod) = %(lastmod)s;\n",
    "        \"\"\",\n",
    "            {\n",
    "                's_filename': XML_gz_Dataset['parent_gz'].drop_duplicates()[0]\n",
    "                ,'lastmod' : XML_gz_Dataset['lastmod'].dt.date.drop_duplicates()[0]\n",
    "            }\n",
    "        )\n",
    "    result = cursor.fetchone()\n",
    "    print(\"parent file link is:\",ScrapeFile,\"is:\", result[0])\n",
    "    XML_gz_Dataset['s_fileid']=result[0]\n",
    "    #remove redundant link\n",
    "    XML_gz_Dataset=XML_gz_Dataset.drop(columns=['parent_gz'])\n",
    "\n",
    "    #time to insert  \n",
    "    print(\"inserting into tables: sc_property_links\")\n",
    "    engine = create_engine('postgresql://postgres:root@172.22.114.65:5432/scrape_db')\n",
    "    XML_gz_Dataset.to_sql(\n",
    "        name='sc_property_links'\n",
    "        ,schema='sc_land'\n",
    "        ,con=engine\n",
    "        ,method=psql_insert_copy\n",
    "        ,if_exists='append'\n",
    "        ,index=False\n",
    "        )\n",
    "    print(\"insert complete\")\n",
    "    print('removing extracted xml file')\n",
    "    os.remove(gz_save_path + _xml_save)\n",
    "    print(\"fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseurl='https://www.realestate.com.au/xml-sitemap/'\n",
    "Scrapewait=5\n",
    "PageSaveFolder= 'C:/Users/chapo/Downloads/xml_output/'\n",
    "ScrapeFile='pdp-sitemap-buy-1.xml.gz'\n",
    "# SaveScrape(baseurl, PageSaveFolder, ScrapeFile, Scrapewait):\n",
    "# print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "realIP is:  27.33.135.59  - proxy IP is: 103.152.5.2  - attempt no. 0\nfin, total time 0.125\nrealIP is:  27.33.135.59  - proxy IP is: 103.152.5.2  - attempt no. 1\n"
     ]
    }
   ],
   "source": [
    "ua = UserAgent()\n",
    "headers = {'User-Agent':str(ua.random)}\n",
    "proxies,external_ip=getProxy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "_getPass=False\n",
    "    #loop until it works, if it takes too long get another proxy \n",
    "while _getPass==False:\n",
    "    try:\n",
    "        response = requests.get(baseurl + ScrapeFile, headers=headers,proxies=proxies, timeout=Scrapewait)\n",
    "        _getPass=True\n",
    "    except Exception as e:\n",
    "        print('error recieved, trying again:',e) \n",
    "        proxies,external_ip=getProxy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gz file: pdp-sitemap-buy-1.xml.gz\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "594113"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "print('gz file:', ScrapeFile)\n",
    "#download from sitemap, use dynamic variable \n",
    "sitemap_url = baseurl #'https://www.realestate.com.au/xml-sitemap/'#pdp-sitemap-buy-1.xml.gz' \n",
    "_file=ScrapeFile #im lazy, sue me\n",
    "gz_save_name =_file[:-7] + '_' + (datetime.datetime.now()).strftime('%Y-%m-%d') + '.gz'\n",
    "gz_url = sitemap_url + _file\n",
    "gz_save_path = PageSaveFolder\n",
    "#save to gz\n",
    "open(gz_save_path + gz_save_name, 'wb').write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "file: pdp-sitemap-buy-1_2021-01-07.gz\nwritten to dir: C:/Users/chapo/Downloads/xml_output/pdp-sitemap-buy-1_2021-01-07.gz\n"
     ]
    }
   ],
   "source": [
    "#save gz to dir for archiving \n",
    "print(\"file:\", gz_save_name)\n",
    "print(\"written to dir:\", gz_save_path + gz_save_name)\n",
    "#feast upon that rich gooey xml \n",
    "_xml_save = _file[:-7] + '_' + (datetime.datetime.now()).strftime('%Y-%m-%d') + '.xml'  \n",
    "with gzip.open(gz_save_path + gz_save_name, 'rb') as f_in:\n",
    "    with open(gz_save_path + _xml_save, 'wb') as f_out: \n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "tree = etree.parse(gz_save_path + _xml_save)\n",
    "with open(gz_save_path + _xml_save, \"wb\") as saveXML:\n",
    "    saveXML.write(etree.tostring(tree,pretty_print=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "XML_gz_Dataset=pd.DataFrame(columns =['parent_gz','scrape_dt','url', 'proptype', 'state', 'suburb', 'prop_id', 'lastmod', 'external_ip', 's_fileid'])\n",
    "# _PropType=_State=_PropID=_LastMod=_split=_Url=\"\"\n",
    "# _external_ip = urllib.request.urlopen('https://ident.me').read().decode('utf8')\n",
    "_count=1\n",
    "_time=time.time()\n",
    "#iterate xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "body=tree.xpath('//ns:url',namespaces={'ns':\"http://www.sitemaps.org/schemas/sitemap/0.9\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "interval: 9999  -total runtime: 837.5966055393219\n",
      "interval: 19999  -total runtime: 1146.3312232494354\n",
      "interval: 29999  -total runtime: 1506.55140042305\n",
      "interval: 39999  -total runtime: 1901.5740237236023\n",
      "interval: 49999  -total runtime: 2375.631979703903\n"
     ]
    }
   ],
   "source": [
    "for element in body:\n",
    "    if _count % 10000 == 0: \n",
    "        print(\"interval:\", str(_count-1),\" -total runtime:\", time.time()-_time)\n",
    "    _LastMod = element[1].text\n",
    "    _Url = element[0].text\n",
    "    #everyintg below here is derived\n",
    "    if '-tas-' in element[0].text: \n",
    "        _State='tas'\n",
    "    elif '-vic-' in element[0].text: \n",
    "        _State='vic'\n",
    "    elif '-nsw-' in element[0].text: \n",
    "        _State='nsw'\n",
    "    elif '-act-' in element[0].text: \n",
    "        _State='act'\n",
    "    elif '-qld-' in element[0].text: \n",
    "        _State='qld'\n",
    "    elif '-nt-' in element[0].text: \n",
    "        _State='nt'\n",
    "    elif '-sa-' in element[0].text: \n",
    "        _State='sa'\n",
    "    elif '-wa-' in element[0].text: \n",
    "        _State='wa'\n",
    "\n",
    "    if _State=='': #sometimes the urls they give are wrong\n",
    "        print(\"incorrect url:\", str(element[0].text))\n",
    "        _PropType=''\n",
    "        _Suburb=''\n",
    "        _PropID=''\n",
    "    else: \n",
    "        _split=str(element[0].text).split(_State) \n",
    "        #had to do it this way so unconventional suburb names are still caught\n",
    "        _PropType = _split[0].replace('https://www.realestate.com.au/property-','')[:-1]\n",
    "        _split=str(element[0].text).split('-')\n",
    "        _Suburb=_split[len(_split) -2 ]\n",
    "        _PropID=_split[len(_split) -1 ]\n",
    "\n",
    "    #writes results to df, same as the previous module \n",
    "    XML_gz_Dataset=XML_gz_Dataset.append({\n",
    "                'parent_gz': str(ScrapeFile)\n",
    "                ,'scrape_dt' : (datetime.datetime.now()).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                , 'url' : str(_Url)\n",
    "                , 'proptype' : str(_PropType)\n",
    "                , 'state' : str(_State)\n",
    "                , 'suburb' : str(_Suburb)\n",
    "                , 'prop_id' : str(_PropID)\n",
    "                , 'lastmod': str(_LastMod)\n",
    "                , 'external_ip': str(external_ip)\n",
    "                } ,ignore_index=True) \n",
    "    _PropType=_State=_PropID=_LastMod=_split=_Url=\"\"\n",
    "    _count+=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "file saved to: C:/Users/chapo/Downloads/xml_output/\\parsed_csv\\pdp-sitemap-buy-1_2021-01-07._results.csv\n"
     ]
    }
   ],
   "source": [
    "XML_gz_Dataset.to_csv(gz_save_path + '/parsed_csv/' + _xml_save[:-3] + '_results' +'.csv')\n",
    "print(\"file saved to: \" + gz_save_path + '\\\\parsed_csv\\\\' + _xml_save[:-3] + '_results' +'.csv')\n",
    "XML_gz_Dataset['lastmod']=pd.to_datetime(XML_gz_Dataset['lastmod'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "parent file link is: pdp-sitemap-buy-1.xml.gz is: 7914\n",
      "inserting into tables: sc_property_links\n",
      "insert complete\n",
      "removing extracted xml file\n",
      "fin\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#now we add to db table \n",
    "#parent file link\n",
    "connection = psycopg2.connect(user=\"postgres\",password=\"root\",host=\"172.22.114.65\",port=\"5432\",database=\"scrape_db\")\n",
    "cursor = connection.cursor()\n",
    "# with connection.cursor() as cursor:\n",
    "cursor.execute(\"\"\"\n",
    "    select max(s_fileid)\n",
    "    FROM sc_land.sc_source_file\n",
    "    WHERE s_filename = %(s_filename)s\n",
    "    and date(lastmod) = %(lastmod)s;\n",
    "    \"\"\",\n",
    "        {\n",
    "            's_filename': XML_gz_Dataset['parent_gz'].drop_duplicates()[0]\n",
    "            ,'lastmod' : XML_gz_Dataset['lastmod'].dt.date.drop_duplicates()[0]\n",
    "        }\n",
    "    )\n",
    "result = cursor.fetchone()\n",
    "print(\"parent file link is:\",ScrapeFile,\"is:\", result[0])\n",
    "XML_gz_Dataset['s_fileid']=result[0]\n",
    "#remove redundant link\n",
    "XML_gz_Dataset=XML_gz_Dataset.drop(columns=['parent_gz'])\n",
    "\n",
    "#time to insert  \n",
    "print(\"inserting into tables: sc_property_links\")\n",
    "engine = create_engine('postgresql://postgres:root@172.22.114.65:5432/scrape_db')\n",
    "XML_gz_Dataset.to_sql(\n",
    "    name='sc_property_links'\n",
    "    ,schema='sc_land'\n",
    "    ,con=engine\n",
    "    ,method=psql_insert_copy\n",
    "    ,if_exists='append'\n",
    "    ,index=False\n",
    "    )\n",
    "print(\"insert complete\")\n",
    "print('removing extracted xml file')\n",
    "os.remove(gz_save_path + _xml_save)\n",
    "print(\"fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "interval: 9999  -total runtime: 38.58593797683716\n",
      "interval: 19999  -total runtime: 102.50521445274353\n",
      "interval: 29999  -total runtime: 199.58117198944092\n",
      "interval: 39999  -total runtime: 334.2834167480469\n",
      "interval: 49999  -total runtime: 500.5072994232178\n"
     ]
    }
   ],
   "source": [
    "_count=1\n",
    "_time=time.time()\n",
    "new_Dataset=pd.DataFrame(columns =['parent_gz','scrape_dt','url', 'lastmod', 'external_ip', 's_fileid'])\n",
    "for element in body:\n",
    "    if _count % 10000 == 0: \n",
    "        print(\"interval:\", str(_count-1),\" -total runtime:\", time.time()-_time)\n",
    "    # _LastMod = element[1].text\n",
    "    # _Url = element[0].text\n",
    "    # #writes results to df, same as the previous module \n",
    "    new_Dataset=new_Dataset.append({\n",
    "                'parent_gz': str(ScrapeFile)\n",
    "                ,'scrape_dt' : (datetime.datetime.now()).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                , 'url' : str(element[0].text)\n",
    "                , 'lastmod': str(element[1].text)\n",
    "                , 'external_ip': str(external_ip)\n",
    "                } ,ignore_index=True) \n",
    "    _count+=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=new_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "parent_gz      object\n",
       "scrape_dt      object\n",
       "url            object\n",
       "lastmod        object\n",
       "external_ip    object\n",
       "s_fileid       object\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add state\n",
    "df['state']=df.apply(lambda x: \n",
    "    'nsw' if '-nsw-' in x.url else\n",
    "    'qld' if '-qld-' in x.url else\n",
    "    'tas' if '-tas-' in x.url else\n",
    "    'act' if '-act-' in x.url else\n",
    "    'sa' if '-sa-' in x.url else\n",
    "    'nt' if '-nt-' in x.url else\n",
    "    'wa' if '-wa-' in x.url else\n",
    "    'vic' if '-vic-' in x.url else ''\n",
    "    , axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get proptype \n",
    "df['proptype']=df['url'].apply(lambda x: \n",
    "    x.split('-nsw-')[0].replace('https://www.realestate.com.au/property-','')if len(x.split('-nsw-')) > 1 else \n",
    "    x.split('-qld-')[0].replace('https://www.realestate.com.au/property-','') if len(x.split('-qld-')) > 1 else \n",
    "    x.split('-tas-')[0].replace('https://www.realestate.com.au/property-','') if len(x.split('-tas-')) > 1 else \n",
    "    x.split('-act-')[0].replace('https://www.realestate.com.au/property-','') if len(x.split('-act-')) > 1 else \n",
    "    x.split('-sa-')[0].replace('https://www.realestate.com.au/property-','') if len(x.split('-sa-')) > 1 else \n",
    "    x.split('-nt-')[0].replace('https://www.realestate.com.au/property-','') if len(x.split('-nt-')) > 1 else \n",
    "    x.split('-wa-')[0].replace('https://www.realestate.com.au/property-','') if len(x.split('-wa-')) > 1 else \n",
    "    x.split('-vic-')[0].replace('https://www.realestate.com.au/property-','') if len(x.split('-vic-')) > 1 else ''\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['suburb']=df['url'].apply(lambda x:\n",
    "#     x.split('-')[3].replace('https://www.realestate.com.au/property','').replace('nsw','') if len(x.split('nsw')) > 1 else\n",
    "#     x.split('-')[3].replace('https://www.realestate.com.au/property','').replace('qld','') if len(x.split('qld')) > 1 else \n",
    "#     x.split('-')[3].replace('https://www.realestate.com.au/property','').replace('tas','') if len(x.split('tas')) > 1 else \n",
    "#     x.split('-')[3].replace('https://www.realestate.com.au/property','').replace('act','') if len(x.split('act')) > 1 else \n",
    "#     x.split('-')[3].replace('https://www.realestate.com.au/property','').replace('sa','') if len(x.split('sa')) > 1 else \n",
    "#     x.split('-')[3].replace('https://www.realestate.com.au/property','').replace('nt','') if len(x.split('nt')) > 1 else \n",
    "#     x.split('-')[3].replace('https://www.realestate.com.au/property','').replace('wa','') if len(x.split('wa')) > 1 else \n",
    "#     x.split('-')[3].replace('https://www.realestate.com.au/property','').replace('vic','') if len(x.split('vic')) > 1 else ''\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-206-8f53c15cb972>, line 13)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-206-8f53c15cb972>\"\u001b[1;36m, line \u001b[1;32m13\u001b[0m\n\u001b[1;33m    )\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "df['suburb']=df['url'].apply(lambda x:\n",
    "    # x.split('-')[0:-2]\n",
    "    # ' '.join\n",
    "    sorted(list(set(val))))\n",
    "    # https://stackoverflow.com/questions/58077317/split-a-pandas-column-by-comma-and-for-each-item-split-by-space-and-finally-have\n",
    "    x.split('-')[0:-2].replace('https://www.realestate.com.au/property','').replace('nsw','') if len(x.split('nsw')) > 1 else\n",
    "    # x.split('-')[3].replace('https://www.realestate.com.au/property','').replace('qld','') if len(x.split('qld')) > 1 else \n",
    "    # x.split('-')[3].replace('https://www.realestate.com.au/property','').replace('tas','') if len(x.split('tas')) > 1 else \n",
    "    # x.split('-')[3].replace('https://www.realestate.com.au/property','').replace('act','') if len(x.split('act')) > 1 else \n",
    "    # x.split('-')[3].replace('https://www.realestate.com.au/property','').replace('sa','') if len(x.split('sa')) > 1 else \n",
    "    # x.split('-')[3].replace('https://www.realestate.com.au/property','').replace('nt','') if len(x.split('nt')) > 1 else \n",
    "    # x.split('-')[3].replace('https://www.realestate.com.au/property','').replace('wa','') if len(x.split('wa')) > 1 else \n",
    "    # x.split('-')[3].replace('https://www.realestate.com.au/property','').replace('vic','') if len(x.split('vic')) > 1 else ''\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['prop_id']=df['url'].apply(lambda x:\r\n",
    "    x.split('-')[-1]\r\n",
    "    # .replace('https://www.realestate.com.au/property','').replace('nsw','') if len(x.split('nsw')) > 1 else\r\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                              suburb                                                url\n0  [https://www.realestate.com.au/property, house...  https://www.realestate.com.au/property-house-n...\n1  [https://www.realestate.com.au/property, house...  https://www.realestate.com.au/property-house-v...\n2  [https://www.realestate.com.au/property, apart...  https://www.realestate.com.au/property-apartme...\n3  [https://www.realestate.com.au/property, house...  https://www.realestate.com.au/property-house-q...\n4  [https://www.realestate.com.au/property, house...  https://www.realestate.com.au/property-house-n...\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.width', 2000)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "print(df[['suburb','url']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'C:\\Users\\chapo\\Downloads\\xml_output\\parsed_csv\\test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}