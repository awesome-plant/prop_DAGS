{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fp.fp import FreeProxy\n",
    "import requests \n",
    "from fake_useragent import UserAgent\n",
    "import time \n",
    "import datetime \n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import shutil \n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "from psycopg2 import Error\n",
    "import csv\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psql_insert_copy(table, conn, keys, data_iter):\n",
    "    \"\"\"\n",
    "    Execute SQL statement inserting data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    table : pandas.io.sql.SQLTable\n",
    "    conn : sqlalchemy.engine.Engine or sqlalchemy.engine.Connection\n",
    "    keys : list of str\n",
    "        Column names\n",
    "    data_iter : Iterable that iterates the values to be inserted\n",
    "    \"\"\"\n",
    "    # gets a DBAPI connection that can provide a cursor\n",
    "    dbapi_conn = conn.connection\n",
    "    with dbapi_conn.cursor() as cur:\n",
    "        s_buf = StringIO()\n",
    "        writer = csv.writer(s_buf)\n",
    "        writer.writerows(data_iter)\n",
    "        s_buf.seek(0)\n",
    "\n",
    "        columns = ', '.join('\"{}\"'.format(k) for k in keys)\n",
    "        if table.schema:\n",
    "            table_name = '{}.{}'.format(table.schema, table.name)\n",
    "        else:\n",
    "            table_name = table.name\n",
    "\n",
    "        sql = 'COPY {} ({}) FROM STDIN WITH CSV'.format(\n",
    "            table_name, columns)\n",
    "        cur.copy_expert(sql=sql, file=s_buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProxy(): \n",
    "    # def here returns proxy, confirmed with different whatismyip return \n",
    "    url='https://ident.me/'\n",
    "    ua = UserAgent()\n",
    "    q=requests.get(url)\n",
    "    _actualIP=q.text\n",
    "    _newIP=''\n",
    "    _getIP_time=time.process_time()\n",
    "    _try=0\n",
    "    _checkout=False\n",
    "    while _checkout==False: #_newIP != _actualIP :\n",
    "        headers = {'User-Agent':str(ua.random)}\n",
    "        start= time.process_time()\n",
    "        proxy = FreeProxy(rand=True).get()\n",
    "        taken = time.process_time() - start\n",
    "        proxies= { 'http': proxy, 'https': proxy } \n",
    "        try:\n",
    "            r = requests.get(url, headers=headers, proxies=proxies)\n",
    "            _newIP = r.text\n",
    "            print(\"realIP is: \", _actualIP, \" - proxy IP is:\", _newIP, \" - attempt no.\", str(_try))\n",
    "        except Exception as e: \n",
    "            print('error on proxy get, try again:', e)\n",
    "        if _actualIP !=_newIP:\n",
    "            _checkout=True\n",
    "        _try+=1\n",
    "    print(\"fin, total time\", str( time.process_time() - _getIP_time ) )\n",
    "    print(\"realIP is: \", _actualIP, \" - proxy IP is:\", _newIP, \" - attempt no.\", str(_try))\n",
    "    return proxies, _newIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def SaveScrape(baseurl, PageSaveFolder, ScrapeFile, **kwargs):\n",
    "#     print('gz file:', ScrapeFile)\n",
    "#     #download from sitemap, use dynamic variable \n",
    "#     sitemap_url = baseurl #'https://www.realestate.com.au/xml-sitemap/'#pdp-sitemap-buy-1.xml.gz' \n",
    "#     _file=ScrapeFile #im lazy, sue me\n",
    "#     gz_save_name =_file[:-7] + '_' + (datetime.datetime.now()).strftime('%Y-%m-%d') + '.gz'\n",
    "#     gz_url = sitemap_url + _file\n",
    "#     gz_save_path = PageSaveFolder\n",
    "#     urllib.request.urlretrieve(gz_url, gz_save_path + gz_save_name)\n",
    "\n",
    "#     #save gz to dir for archiving \n",
    "#     print(\"file:\", gz_save_name)\n",
    "#     print(\"written to dir:\", gz_save_path + gz_save_name)\n",
    "#     #feast upon that rich gooey xml \n",
    "#     _xml_save = _file[:-7] + '_' + (datetime.datetime.now()).strftime('%Y-%m-%d') + '.xml'  \n",
    "#     with gzip.open(gz_save_path + gz_save_name, 'rb') as f_in:\n",
    "#         with open(gz_save_path + _xml_save, 'wb') as f_out: \n",
    "#             shutil.copyfileobj(f_in, f_out)\n",
    "#     #xml part \n",
    "#     root = etree.parse(gz_save_path + _xml_save)\n",
    "#     XML_gz_Dataset=pd.DataFrame(columns =['parent_gz','scrape_dt','url', 'proptype', 'state', 'suburb', 'prop_id', 'lastmod', 'external_ip', 's_fileid'])\n",
    "#     _PropType=_State=_PropID=_LastMod=_split=_Url=\"\"\n",
    "#     _external_ip = urllib.request.urlopen('https://ident.me').read().decode('utf8')\n",
    "#     _count=1\n",
    "#     _time=time.time()\n",
    "#     #iterate xml\n",
    "#     for element in root.iter():\n",
    "#         if _count % 10000 == 0: \n",
    "#             print(\"interval:\", str(_count-1),\" -total runtime:\", time.time()-_time)\n",
    "#         #writes results to df, same as the previous module \n",
    "#         if 'url' in element.tag and _Url != '':\n",
    "#             XML_gz_Dataset=XML_gz_Dataset.append({\n",
    "#                         'parent_gz': str(ScrapeFile)\n",
    "#                         ,'scrape_dt' : (datetime.datetime.now()).strftime('%Y-%m-%d %H:%M:%S')\n",
    "#                         , 'url' : str(_Url)\n",
    "#                         , 'proptype' : str(_PropType)\n",
    "#                         , 'state' : str(_State)\n",
    "#                         , 'suburb' : str(_Suburb)\n",
    "#                         , 'prop_id' : str(_PropID)\n",
    "#                         , 'lastmod': str(_LastMod)\n",
    "#                         , 'external_ip': str(_external_ip)\n",
    "#                         } ,ignore_index=True) \n",
    "#             _PropType=_State=_PropID=_LastMod=_split=_Url=\"\"\n",
    "#         if 'lastmod' in element.tag: \n",
    "#             _LastMod = element.text\n",
    "#         #just about everything gleaned from loac (url) tag\n",
    "#         elif 'loc' in element.tag: \n",
    "#             if '-tas-' in element.text: \n",
    "#                 _State='tas'\n",
    "#             elif '-vic-' in element.text: \n",
    "#                 _State='vic'\n",
    "#             elif '-nsw-' in element.text: \n",
    "#                 _State='nsw'\n",
    "#             elif '-act-' in element.text: \n",
    "#                 _State='act'\n",
    "#             elif '-qld-' in element.text: \n",
    "#                 _State='qld'\n",
    "#             elif '-nt-' in element.text: \n",
    "#                 _State='nt'\n",
    "#             elif '-sa-' in element.text: \n",
    "#                 _State='sa'\n",
    "#             elif '-wa-' in element.text: \n",
    "#                 _State='wa'\n",
    "    \n",
    "#             _Url = element.text\n",
    "#             if _State=='': #sometimes the urls they give are wrong\n",
    "#                 print(\"incorrect url:\", str(element.text))\n",
    "#                 _PropType=''\n",
    "#                 _Suburb=''\n",
    "#                 _PropID=''\n",
    "#             else: \n",
    "#                 _split=str(element.text).split(_State) \n",
    "#                 #had to do it this way so unconventional suburb names are still caught\n",
    "#                 _PropType = _split[0].replace('https://www.realestate.com.au/property-','')[:-1]\n",
    "#                 _split=str(element.text).split('-')\n",
    "#                 _Suburb=_split[len(_split) -2 ]\n",
    "#                 _PropID=_split[len(_split) -1 ]\n",
    "#             _count+=1\n",
    "#     XML_gz_Dataset.to_csv(gz_save_path + '\\\\parsed_csv\\\\' + _xml_save[:-3] + '_results' +'.csv')\n",
    "#     print(\"file saved to: \" + gz_save_path + '\\\\parsed_csv\\\\' + _xml_save[:-3] + '_results' +'.csv')\n",
    "#     XML_gz_Dataset['lastmod']=pd.to_datetime(XML_gz_Dataset['lastmod'])\n",
    "#     #now we add to db table \n",
    "#     #parent file link\n",
    "#     connection = psycopg2.connect(user=\"postgres\",password=\"root\",host=\"172.22.114.65\",port=\"5432\",database=\"scrape_db\")\n",
    "#     cursor = connection.cursor()\n",
    "#     # with connection.cursor() as cursor:\n",
    "#     cursor.execute(\"\"\"\n",
    "#         select max(s_fileid)\n",
    "#         FROM sc_land.sc_source_file\n",
    "#         WHERE s_filename = %(s_filename)s\n",
    "#         and date(lastmod) = %(lastmod)s;\n",
    "#         \"\"\",\n",
    "#             {\n",
    "#                 's_filename': XML_gz_Dataset['parent_gz'].drop_duplicates()[0]\n",
    "#                 ,'lastmod' : XML_gz_Dataset['lastmod'].dt.date.drop_duplicates()[0]\n",
    "#             }\n",
    "#         )\n",
    "#     result = cursor.fetchone()\n",
    "#     print(\"parent file link is:\",ScrapeFile,\"is:\", result[0])\n",
    "#     XML_gz_Dataset['s_fileid']=result[0]\n",
    "#     #remove redundant link\n",
    "#     XML_gz_Dataset=XML_gz_Dataset.drop(columns=['parent_gz'])\n",
    "\n",
    "#     #time to insert  \n",
    "#     print(\"inserting into tables: sc_property_links\")\n",
    "#     engine = create_engine('postgresql://postgres:root@172.22.114.65:5432/scrape_db')\n",
    "#     XML_gz_Dataset.to_sql(\n",
    "#         name='sc_property_links'\n",
    "#         ,schema='sc_land'\n",
    "#         ,con=engine\n",
    "#         ,method=psql_insert_copy\n",
    "#         ,if_exists='append'\n",
    "#         ,index=False\n",
    "#         )\n",
    "#     print(\"insert complete\")\n",
    "#     print('removing extracted xml file')\n",
    "#     os.remove(gz_save_path + _xml_save)\n",
    "#     print(\"fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseurl='https://www.realestate.com.au/xml-sitemap/'\n",
    "Scrapewait=5\n",
    "PageSaveFolder= 'C:/Users/chapo/Downloads/xml_output/'\n",
    "ScrapeFile='pdp-sitemap-buy-1.xml.gz'\n",
    "# SaveScrape(baseurl, PageSaveFolder, ScrapeFile, Scrapewait):\n",
    "# print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "error on proxy get, try again: HTTPSConnectionPool(host='ident.me', port=443): Max retries exceeded with url: / (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 403 Forbidden')))\n",
      "fin, total time 0.046875\n",
      "realIP is:  27.33.135.59  - proxy IP is:   - attempt no. 1\n",
      "error recieved, trying again: HTTPSConnectionPool(host='www.realestate.com.au', port=443): Max retries exceeded with url: /xml-sitemap/pdp-sitemap-buy-1.xml.gz (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 403 Forbidden')))\n",
      "error on proxy get, try again: HTTPSConnectionPool(host='ident.me', port=443): Max retries exceeded with url: / (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 403 Forbidden')))\n",
      "fin, total time 0.03125\n",
      "realIP is:  27.33.135.59  - proxy IP is:   - attempt no. 1\n",
      "error recieved, trying again: HTTPSConnectionPool(host='www.realestate.com.au', port=443): Max retries exceeded with url: /xml-sitemap/pdp-sitemap-buy-1.xml.gz (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 403 Forbidden')))\n",
      "realIP is:  27.33.135.59  - proxy IP is: 103.152.119.231  - attempt no. 0\n",
      "fin, total time 0.140625\n",
      "realIP is:  27.33.135.59  - proxy IP is: 103.152.119.231  - attempt no. 1\n",
      "gz file: pdp-sitemap-buy-1.xml.gz\n",
      "file: pdp-sitemap-buy-1_2021-01-08.gz\n",
      "written to dir: C:/Users/chapo/Downloads/xml_output/pdp-sitemap-buy-1_2021-01-08.gz\n",
      "interval: 9999  -total runtime: 36.6220064163208\n",
      "interval: 19999  -total runtime: 96.68260383605957\n",
      "interval: 29999  -total runtime: 193.90656685829163\n",
      "interval: 39999  -total runtime: 321.11733746528625\n",
      "interval: 49999  -total runtime: 475.3226909637451\n",
      "xml extract time: 475.3399999141693\n",
      "file saved to: C:/Users/chapo/Downloads/xml_output/\\parsed_csv\\pdp-sitemap-buy-1_2021-01-08._results.csv\n",
      "total time: 478.54807591438293\n",
      "parent file link is: pdp-sitemap-buy-1.xml.gz is: None\n",
      "inserting into tables: sc_property_links\n",
      "insert complete\n",
      "removing extracted xml file\n",
      "fin\n"
     ]
    }
   ],
   "source": [
    "    ua = UserAgent()\n",
    "    headers = {'User-Agent':str(ua.random)}\n",
    "    proxies,external_ip=getProxy()\n",
    "\n",
    "    _getPass=False\n",
    "        #loop until it works, if it takes too long get another proxy \n",
    "    while _getPass==False:\n",
    "        try:\n",
    "            response = requests.get(baseurl + ScrapeFile, headers=headers,proxies=proxies, timeout=Scrapewait)\n",
    "            _getPass=True\n",
    "        except Exception as e:\n",
    "            print('error recieved, trying again:',e) \n",
    "            proxies,external_ip=getProxy()\n",
    "\n",
    "    print('gz file:', ScrapeFile)\n",
    "    #download from sitemap, use dynamic variable \n",
    "    sitemap_url = baseurl #'https://www.realestate.com.au/xml-sitemap/'#pdp-sitemap-buy-1.xml.gz' \n",
    "    _file=ScrapeFile #im lazy, sue me\n",
    "    gz_save_name =_file[:-7] + '_' + (datetime.datetime.now()).strftime('%Y-%m-%d') + '.gz'\n",
    "    gz_url = sitemap_url + _file\n",
    "    gz_save_path = PageSaveFolder\n",
    "    #save to gz\n",
    "    open(gz_save_path + gz_save_name, 'wb').write(response.content)\n",
    "\n",
    "    #save gz to dir for archiving \n",
    "    print(\"file:\", gz_save_name)\n",
    "    print(\"written to dir:\", gz_save_path + gz_save_name)\n",
    "    #feast upon that rich gooey xml \n",
    "    _xml_save = _file[:-7] + '_' + (datetime.datetime.now()).strftime('%Y-%m-%d') + '.xml'  \n",
    "    with gzip.open(gz_save_path + gz_save_name, 'rb') as f_in:\n",
    "        with open(gz_save_path + _xml_save, 'wb') as f_out: \n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "    tree = etree.parse(gz_save_path + _xml_save)\n",
    "    with open(gz_save_path + _xml_save, \"wb\") as saveXML:\n",
    "        saveXML.write(etree.tostring(tree,pretty_print=True))\n",
    "\n",
    "    _count=1\n",
    "    _time=time.time()\n",
    "    #iterate xml\n",
    "    body=tree.xpath('//ns:url',namespaces={'ns':\"http://www.sitemaps.org/schemas/sitemap/0.9\"})\n",
    "    _count=1\n",
    "    _time=time.time()\n",
    "    # XML_gz_Dataset=pd.DataFrame(columns =['parent_gz','scrape_dt','url', 'proptype', 'state', 'suburb', 'prop_id', 'lastmod', 'external_ip', 's_fileid'])\n",
    "    new_Dataset=   pd.DataFrame(columns =['parent_gz','scrape_dt','url', 'lastmod', 's_fileid'])\n",
    "    for element in body:\n",
    "        if _count % 10000 == 0: \n",
    "            print(\"interval:\", str(_count-1),\" -total runtime:\", time.time()-_time)\n",
    "        # _LastMod = element[1].text\n",
    "        # _Url = element[0].text\n",
    "        # #writes results to df, same as the previous module \n",
    "        new_Dataset=new_Dataset.append({\n",
    "                    'parent_gz': str(ScrapeFile)\n",
    "                    ,'scrape_dt' : (datetime.datetime.now()).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    , 'url' : str(element[0].text)\n",
    "                    , 'lastmod': str(element[1].text)\n",
    "                    } ,ignore_index=True) \n",
    "        _count+=1 \n",
    "    print(\"xml extract time:\", time.time() - _time)\n",
    "    XML_gz_Dataset=new_Dataset\n",
    "\n",
    "    #add state\n",
    "    XML_gz_Dataset['state']=XML_gz_Dataset.apply(lambda x: \n",
    "        'nsw' if '-nsw-' in x.url else\n",
    "        'qld' if '-qld-' in x.url else\n",
    "        'tas' if '-tas-' in x.url else\n",
    "        'act' if '-act-' in x.url else\n",
    "        'sa' if '-sa-' in x.url else\n",
    "        'nt' if '-nt-' in x.url else\n",
    "        'wa' if '-wa-' in x.url else\n",
    "        'vic' if '-vic-' in x.url else ''\n",
    "        , axis=1)\n",
    "\n",
    "    # get proptype \n",
    "    XML_gz_Dataset['proptype']=XML_gz_Dataset['url'].apply(lambda x: \n",
    "        x.split('-nsw-')[0].replace('https://www.realestate.com.au/property-','').replace('+', ' ') if len(x.split('-nsw-')) > 1 else \n",
    "        x.split('-qld-')[0].replace('https://www.realestate.com.au/property-','').replace('+', ' ') if len(x.split('-qld-')) > 1 else \n",
    "        x.split('-tas-')[0].replace('https://www.realestate.com.au/property-','').replace('+', ' ') if len(x.split('-tas-')) > 1 else \n",
    "        x.split('-act-')[0].replace('https://www.realestate.com.au/property-','').replace('+', ' ') if len(x.split('-act-')) > 1 else \n",
    "        x.split('-sa-')[0].replace('https://www.realestate.com.au/property-','').replace('+', ' ') if len(x.split('-sa-')) > 1 else \n",
    "        x.split('-nt-')[0].replace('https://www.realestate.com.au/property-','').replace('+', ' ') if len(x.split('-nt-')) > 1 else \n",
    "        x.split('-wa-')[0].replace('https://www.realestate.com.au/property-','').replace('+', ' ') if len(x.split('-wa-')) > 1 else \n",
    "        x.split('-vic-')[0].replace('https://www.realestate.com.au/property-','').replace('+', ' ') if len(x.split('-vic-')) > 1 else ''\n",
    "        )\n",
    "\n",
    "    #get suburb\n",
    "    XML_gz_Dataset['suburb']=XML_gz_Dataset['url'].apply(lambda x:\n",
    "        x.split('-nsw-')[1].replace('https://www.realestate.com.au/property-','').replace(x.split('-')[-1],'').replace('-',' ').replace('+', ' ').strip() if len(x.split('-nsw-')) > 1 else\n",
    "        x.split('-qld-')[1].replace('https://www.realestate.com.au/property-','').replace(x.split('-')[-1],'').replace('-',' ').replace('+', ' ').strip() if len(x.split('-qld-')) > 1 else\n",
    "        x.split('-tas-')[1].replace('https://www.realestate.com.au/property-','').replace(x.split('-')[-1],'').replace('-',' ').replace('+', ' ').strip() if len(x.split('-tas-')) > 1 else\n",
    "        x.split('-act-')[1].replace('https://www.realestate.com.au/property-','').replace(x.split('-')[-1],'').replace('-',' ').replace('+', ' ').strip() if len(x.split('-act-')) > 1 else\n",
    "        x.split('-sa-')[1].replace('https://www.realestate.com.au/property-','').replace(x.split('-')[-1],'').replace('-',' ').replace('+', ' ').strip() if len(x.split('-sa-')) > 1 else\n",
    "        x.split('-nt-')[1].replace('https://www.realestate.com.au/property-','').replace(x.split('-')[-1],'').replace('-',' ').replace('+', ' ').strip() if len(x.split('-nt-')) > 1 else\n",
    "        x.split('-wa-')[1].replace('https://www.realestate.com.au/property-','').replace(x.split('-')[-1],'').replace('-',' ').replace('+', ' ').strip() if len(x.split('-wa-')) > 1 else\n",
    "        x.split('-vic-')[1].replace('https://www.realestate.com.au/property-','').replace(x.split('-')[-1],'').replace('-',' ').replace('+', ' ').strip() if len(x.split('-vic-')) > 1 else ''\n",
    "        )\n",
    "\n",
    "    #get prop id\n",
    "    XML_gz_Dataset['prop_id']=XML_gz_Dataset['url'].apply(lambda x:\n",
    "        x.split('-')[-1]\n",
    "    )\n",
    "    XML_gz_Dataset.to_csv(gz_save_path + '/parsed_csv/' + _xml_save[:-3] + '_results' +'.csv')\n",
    "    print(\"file saved to: \" + gz_save_path + '\\\\parsed_csv\\\\' + _xml_save[:-3] + '_results' +'.csv')\n",
    "    XML_gz_Dataset['lastmod']=pd.to_datetime(XML_gz_Dataset['lastmod'])\n",
    "    print(\"total time:\", time.time() - _time)\n",
    "\n",
    "\n",
    "    #now we add to db table \n",
    "    #parent file link\n",
    "    connection = psycopg2.connect(user=\"postgres\",password=\"root\",host=\"172.22.114.65\",port=\"5432\",database=\"scrape_db\")\n",
    "    cursor = connection.cursor()\n",
    "    # with connection.cursor() as cursor:\n",
    "    cursor.execute(\"\"\"\n",
    "        select max(s_fileid)\n",
    "        FROM sc_land.sc_source_file\n",
    "        WHERE s_filename = %(s_filename)s\n",
    "        and date(lastmod) = %(lastmod)s;\n",
    "        \"\"\",\n",
    "            {\n",
    "                's_filename': XML_gz_Dataset['parent_gz'].drop_duplicates()[0]\n",
    "                ,'lastmod' : XML_gz_Dataset['lastmod'].dt.date.drop_duplicates()[0]\n",
    "            }\n",
    "        )\n",
    "    result = cursor.fetchone()\n",
    "    print(\"parent file link is:\",ScrapeFile,\"is:\", result[0])\n",
    "    XML_gz_Dataset['s_fileid']=result[0]\n",
    "    #remove redundant link\n",
    "    XML_gz_Dataset=XML_gz_Dataset.drop(columns=['parent_gz'])\n",
    "\n",
    "    #time to insert  \n",
    "    print(\"inserting into tables: sc_property_links\")\n",
    "    engine = create_engine('postgresql://postgres:root@172.22.114.65:5432/scrape_db')\n",
    "    XML_gz_Dataset.to_sql(\n",
    "        name='sc_property_links'\n",
    "        ,schema='sc_land'\n",
    "        ,con=engine\n",
    "        ,method=psql_insert_copy\n",
    "        ,if_exists='append'\n",
    "        ,index=False\n",
    "        )\n",
    "    print(\"insert complete\")\n",
    "    print('removing extracted xml file')\n",
    "    os.remove(gz_save_path + _xml_save)\n",
    "    print(\"fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "error recieved, trying again: HTTPSConnectionPool(host='www.realestate.com.au', port=443): Max retries exceeded with url: /xml-sitemap/pdp-sitemap-buy-1.xml.gz (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 403 Forbidden')))\n",
      "error on proxy get, try again: HTTPSConnectionPool(host='ident.me', port=443): Max retries exceeded with url: / (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 403 Forbidden')))\n",
      "fin, total time 0.0625\n",
      "realIP is:  27.33.135.59  - proxy IP is:   - attempt no. 1\n",
      "error recieved, trying again: HTTPSConnectionPool(host='www.realestate.com.au', port=443): Max retries exceeded with url: /xml-sitemap/pdp-sitemap-buy-1.xml.gz (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 403 Forbidden')))\n",
      "realIP is:  27.33.135.59  - proxy IP is: 139.99.105.5  - attempt no. 0\n",
      "fin, total time 0.09375\n",
      "realIP is:  27.33.135.59  - proxy IP is: 139.99.105.5  - attempt no. 1\n",
      "error recieved, trying again: HTTPSConnectionPool(host='www.realestate.com.au', port=443): Max retries exceeded with url: /xml-sitemap/pdp-sitemap-buy-1.xml.gz (Caused by ProxyError('Cannot connect to proxy.', RemoteDisconnected('Remote end closed connection without response')))\n",
      "realIP is:  27.33.135.59  - proxy IP is: 45.82.245.34  - attempt no. 0\n",
      "fin, total time 0.125\n",
      "realIP is:  27.33.135.59  - proxy IP is: 45.82.245.34  - attempt no. 1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(r'C:\\Users\\chapo\\Downloads\\xml_output\\parsed_csv\\test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XML_gz_Dataset=pd.DataFrame(columns =['parent_gz','scrape_dt','url', 'proptype', 'state', 'suburb', 'prop_id', 'lastmod', 'external_ip', 's_fileid'])"
   ]
  }
 ]
}